{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is all the equations needed for building Neural Network\n",
    "#input= X, matrix [nx, number of dataset or m]\n",
    "#output=yhat, matrix[nx of output, number of dataset or m]\n",
    "\"\"\"these are the only functions we need to enter for the model:\n",
    "init_parameters_deep(layers_dims)\n",
    "forward_propagation_deep(X, parameters, activation1, activation_final)\n",
    "compute_cost(AL,Y)\n",
    "back_propagation_deep(X,Y, AL, caches, activation1, activation_final)\n",
    "update_parameters(parameters,grads,learning_rate)\n",
    "\"\"\"\n",
    "#hyperparameter= nodes, L-layer, iterations, learning_rate, activation function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    #return result and cache\n",
    "    result=1/(1+np.exp(-z))\n",
    "    cache=z\n",
    "    return result, cache\n",
    "def relu(z):\n",
    "    #return result and cache\n",
    "    result=np.maximum(0,z)\n",
    "    cache=z\n",
    "    return result, cache\n",
    "def tanh(z):\n",
    "    #return result and cache\n",
    "    result=(np.exp(z)-np.exp(-z))/(np.exp(z)+np.exp(-z))\n",
    "    cache=z\n",
    "    return result, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_parameters_deep_normal(layers_dims):\n",
    "    #layers_dims is number of nodes in each layer, start from input until AL(y-hat) layer\n",
    "    #input in list\n",
    "    L=len(layers_dims)-1 #L = number of NN layers, -1 because counted as 0\n",
    "    parameters={}\n",
    "    for l in range(L):\n",
    "        parameters[f\"W{l+1}\"]=np.random.randn(layers_dims[l+1],layers_dims[l])*0.01\n",
    "        parameters[f\"b{l+1}\"]=np.zeros((layers_dims[l+1],1))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_parameters_deep_he(layers_dims):\n",
    "    #to anticipate vanishing/exploding gradient\n",
    "    #works the best using relu/leaky relu\n",
    "    L=len(layers_dims)\n",
    "    parameters={}\n",
    "    for l in range(1,L):\n",
    "        parameters[f\"W{l}\"]=np.random.randn(layers_dims[l],layers_dims[l-1])*np.sqrt(2/layers_dims[l-1])\n",
    "        parameters[f\"b{l}\"]=np.zeros((layers_dims[l],1))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_linear(A_prev, W, b):\n",
    "    #inputs A, W, and b, returns Z and cache(A,W,b) for back prop\n",
    "    Z=np.dot(W,A_prev)+b\n",
    "    cache=(A_prev,W,b)\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters=init_parameters_deep_normal((12288,20,7,5,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_activation_linear(A_prev, W, b, activation):\n",
    "    \"\"\"inputs previous A, W, b, activation function. \n",
    "    Returns A (current layer) and linear_cache(prev) and activation_cache(current Z)\"\"\" \n",
    "    Z, linear_cache = forward_linear(A_prev,W,b)\n",
    "    if activation==\"relu\":\n",
    "        A, activation_cache=relu(Z)\n",
    "    elif activation==\"sigmoid\":\n",
    "        A, activation_cache=sigmoid(Z)\n",
    "    elif activation==\"tanh\":\n",
    "        A, activation_cache=tanh(Z)\n",
    "    else:\n",
    "        raise ValueError(\"activation function undefined\")\n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache=(linear_cache, activation_cache)\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_deep(X, parameters, activation1, activation_final):\n",
    "    #input X, parameters, layers_dims, activation function. Return y-hat, dictionary-caches (linear(A,W,b) and activation(Z))\n",
    "    #layer 1 to L-1 = Relu, layer sigmoid\n",
    "    L=len(parameters)//2 #number of NN layers\n",
    "    A_prev=X\n",
    "    caches=[]\n",
    "    for l in range(1,L):\n",
    "        A, cache=forward_activation_linear(A_prev, parameters[f\"W{l}\"], parameters[f\"b{l}\"],activation1)\n",
    "        caches.append(cache)\n",
    "        A_prev=A\n",
    "    AL, cache=forward_activation_linear(A_prev, parameters[f\"W{L}\"], parameters[f\"b{L}\"],activation_final)\n",
    "    caches.append(cache)\n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_deep_dropout(X,parameters,activation1,activation_final,keep_prob):\n",
    "    #apply dropout from layer 1 to layer L-1\n",
    "    L=len(parameters)//2\n",
    "    A_prev=X\n",
    "    caches=[]\n",
    "    D_collection=[]\n",
    "    for l in range(1,L):\n",
    "        A, cache=forward_activation_linear(A_prev, parameters[f\"W{l}\"], parameters[f\"b{l}\"],activation1)\n",
    "        D=(np.random.randn(A.shape[0],A.shape[1])<keep_prob).astype(int)\n",
    "        A*=D\n",
    "        D_collection.append(D) #D will be set as every third index of a member in a cache\n",
    "        caches.append(cache)\n",
    "        A/=keep_prob\n",
    "        A_prev=A\n",
    "    AL, cache=forward_activation_linear(A_prev, parameters[f\"W{L}\"], parameters[f\"b{L}\"],activation_final)\n",
    "    caches.append(cache)\n",
    "    return AL, caches, D_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL,Y):\n",
    "    #input AL, Y. Return J (Cost function)\n",
    "    #cost function is logistic regression type\n",
    "    m=AL.shape[1]\n",
    "    cost=(np.dot(Y,np.log(AL).T)+np.dot((1-Y),np.log(1-AL).T))/-m\n",
    "    cost=np.squeeze(cost)\n",
    "    return cost\n",
    "def cost_backward(AL,Y):\n",
    "    Y=Y.reshape(AL.shape) #making sure the shape is the same\n",
    "    dAL=-(np.divide(Y,AL)-np.divide((1-Y),(1-AL)))\n",
    "    return dAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_l2(AL, Y, lambd,parameters,layers_dims):\n",
    "    #input AL, Y. Return J (Cost function)\n",
    "    #cost function is logistic regression\n",
    "    #with l2 regularization, cost function will be added with lambda/2m * sigma of all sum W squared\n",
    "    m=AL.shape[1] #amount of data\n",
    "    initial_cost=compute_cost(AL,Y)\n",
    "    L=len(layers_dims) #number of layer +1\n",
    "    sum_every_layer=0\n",
    "    for l in range(1,L):\n",
    "        sum_every_layer+=np.sum(np.square(parameters[f\"W{l}\"]))\n",
    "    l2_part=lambd/(2*m)*sum_every_layer\n",
    "    l2_cost=initial_cost+l2_part\n",
    "    return l2_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sigmoid, relu, tanh backward\n",
    "#calculate dcost/dz\n",
    "def sigmoid_backward(dA,cache):\n",
    "    z=cache\n",
    "    function=1/(1+np.exp(-z))\n",
    "    backward_function= function*(1-function)*dA\n",
    "    return backward_function #return dcost/dZ\n",
    "def relu_backward(dA,cache):\n",
    "    z=cache\n",
    "    backward_function=np.array(dA, copy=True)\n",
    "    backward_function[z<=0]=0\n",
    "    return backward_function #return dcost/dZ\n",
    "def tanh_backward(dA,cache):\n",
    "    z=cache\n",
    "    function=np.tanh(z)\n",
    "    backward_function=(1-np.power(function,2))*dA\n",
    "    return backward_function #return dcost/dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make dA_prev, db, dw\n",
    "def back_linear(dZ,linear_cache):\n",
    "    #input A_prev, W, b. Return dW, dB, dA_prev\n",
    "    A_prev, W, b=linear_cache #current layer linear cache\n",
    "    m=A_prev.shape[1]\n",
    "    dW=np.dot(dZ,A_prev.T)/m\n",
    "    db=np.sum(dZ, axis=1, keepdims=True)/m\n",
    "    dA_prev=np.dot(W.T,dZ)\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make dA_prev, db, dw\n",
    "def back_linear_l2(dZ,linear_cache,lambd):\n",
    "    #input A_prev, W, b. Return dW, dB, dA_prev\n",
    "    A_prev, W, b=linear_cache #current layer linear cache\n",
    "    m=A_prev.shape[1]\n",
    "    l2_reg=W*lambd/m\n",
    "    dW=np.dot(dZ,A_prev.T)/m\n",
    "    dW+=l2_reg\n",
    "    db=np.sum(dZ, axis=1, keepdims=True)/m\n",
    "    dA_prev=np.dot(W.T,dZ)\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_linear_dropout(dZ, linear_cache, D,keep_prob):\n",
    "    #input A_prev, W, b. Return dW, dB, dA_prev\n",
    "    A_prev, W, b=linear_cache #current layer linear cache\n",
    "    m=A_prev.shape[1]\n",
    "    dW=np.dot(dZ,A_prev.T)/m\n",
    "    db=np.sum(dZ, axis=1, keepdims=True)/m\n",
    "    dA_prev=np.dot(W.T,dZ)\n",
    "    dA_prev*=D\n",
    "    dA_prev/=keep_prob\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make dJ/dA[L-1], dWL, dbL of activation function\n",
    "def back_activation_linear(dA, cache, activation):\n",
    "    #input previous function parameters to compute dZ(prev)\n",
    "    linear_cache, activation_cache=cache\n",
    "    if activation==\"sigmoid\":\n",
    "        dZ=sigmoid_backward(dA,activation_cache)\n",
    "        dA_prev, dW, db= back_linear(dZ,linear_cache)\n",
    "    elif activation==\"relu\":\n",
    "        dZ=relu_backward(dA,activation_cache)\n",
    "        dA_prev, dW, db= back_linear(dZ,linear_cache)\n",
    "    elif activation==\"tanh\":\n",
    "        dZ=tanh_backward(dA,activation_cache)\n",
    "        dA_prev, dW, db= back_linear(dZ,linear_cache)\n",
    "    else:\n",
    "        raise ValueError(\"Activation function not recognized!\")\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_activation_linear_dropout(dA, cache, D, activation,keep_prob):\n",
    "    #input previous function parameters to compute dZ(prev)\n",
    "    linear_cache, activation_cache=cache\n",
    "    if activation==\"sigmoid\":\n",
    "        dZ=sigmoid_backward(dA,activation_cache)\n",
    "        dA_prev, dW, db= back_linear_dropout(dZ, linear_cache, D,keep_prob)\n",
    "    elif activation==\"relu\":\n",
    "        dZ=relu_backward(dA,activation_cache)\n",
    "        dA_prev, dW, db= back_linear_dropout(dZ, linear_cache, D,keep_prob)\n",
    "    elif activation==\"tanh\":\n",
    "        dZ=tanh_backward(dA,activation_cache)\n",
    "        dA_prev, dW, db= back_linear_dropout(dZ, linear_cache, D,keep_prob)\n",
    "    else:\n",
    "        raise ValueError(\"Activation function not recognized!\")\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make dJ/dA[L-1], dWL, dbL of activation function\n",
    "def back_activation_linear_l2(dA, cache, activation,lambd):\n",
    "    #input previous function parameters to compute dZ(prev)\n",
    "    linear_cache, activation_cache=cache\n",
    "    if activation==\"sigmoid\":\n",
    "        dZ=sigmoid_backward(dA,activation_cache)\n",
    "        dA_prev, dW, db= back_linear_l2(dZ,linear_cache,lambd)\n",
    "    elif activation==\"relu\":\n",
    "        dZ=relu_backward(dA,activation_cache)\n",
    "        dA_prev, dW, db= back_linear_l2(dZ,linear_cache,lambd)\n",
    "    elif activation==\"tanh\":\n",
    "        dZ=tanh_backward(dA,activation_cache)\n",
    "        dA_prev, dW, db= back_linear_l2(dZ,linear_cache,lambd)\n",
    "    else:\n",
    "        raise ValueError(\"Activation function not recognized!\")\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation_deep(AL, Y, caches, activation1, activation_final):\n",
    "    #input the AL, Y, caches, activation1 (activation for layer 1 until L-1), activation final(layer L)\n",
    "    #return grads, dictionary of gradient of W, b, A\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL = cost_backward(AL,Y)\n",
    "    \n",
    "    current_cache = caches[-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = back_activation_linear(dAL, current_cache, activation_final)\n",
    "    \n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = back_activation_linear(grads[\"dA\" + str(l + 1)], current_cache, activation1)\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation_deep_l2(AL, Y, caches, activation1,activation_final,lambd,parameters):\n",
    "    grads={}\n",
    "    L=len(parameters)//2#number of layer+1\n",
    "    m=AL.shape[1]\n",
    "    Y=Y.reshape(AL.shape) #just in case Y and AL shape is different\n",
    "    \n",
    "    dAL=cost_backward(AL,Y)\n",
    "    grads[f\"dA{L-1}\"]=dAL\n",
    "    current_cache = caches[-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = back_activation_linear_l2(dAL, current_cache, activation_final,lambd)\n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache=caches[l]\n",
    "        dA_prev,dW,db=back_activation_linear_l2(grads[f\"dA{l+1}\"],current_cache,activation1,lambd)\n",
    "        grads[f\"dA{l}\"]=dA_prev\n",
    "        grads[f\"dW{l+1}\"]=dW\n",
    "        grads[f\"db{l+1}\"]=db\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation_deep_dropout(AL, Y, caches, D_collection, activation1,activation_final,keep_prob):\n",
    "    #input the AL, Y, caches, activation1 (activation for layer 1 until L-1), activation final(layer L)\n",
    "    #return grads, dictionary of gradient of W, b, A\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL = cost_backward(AL,Y)\n",
    "    \n",
    "    current_cache = caches[-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = back_activation_linear(dAL, current_cache, activation_final)\n",
    "    grads[\"dA\"+str(L-1)]*=D_collection[-1]\n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        D=D_collection[l-1]\n",
    "        if l!=0:\n",
    "            dA_prev_temp, dW_temp, db_temp = back_activation_linear_dropout(grads[\"dA\" + str(l+1)], current_cache, D, activation1,keep_prob)\n",
    "        else:\n",
    "            dA_prev_temp, dW_temp, db_temp = back_activation_linear(grads[\"dA\" + str(l + 1)], current_cache, activation1)\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_graddesc(parameters,grads,learning_rate):\n",
    "    L=len(parameters)//2 #number of NN layer, input layer isn't included\n",
    "    #parameters=parameters - alpha*grads\n",
    "    for l in range(1,L+1):\n",
    "        parameters[\"W\"+str(l)]-= learning_rate*grads[\"dW\"+str(l)]\n",
    "        parameters[\"b\"+str(l)]-= learning_rate*grads[\"db\"+str(l)]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_parameters_momentum(parameters):\n",
    "    #initialize and return v\n",
    "    v={}\n",
    "    L=len(parameters)//2\n",
    "    for l in range(1,L+1):\n",
    "        v[f\"W{l}\"]=np.zeros((parameters[f\"W{l}\"].shape[0],parameters[f\"W{l}\"].shape[1]))\n",
    "        v[f\"b{l}\"]=np.zeros((parameters[f\"b{l}\"].shape[0],1))\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_momentum(parameters, grads, learning_rate, v, beta=0.9):\n",
    "    #using higher learning_rate(alpha) is possible now!\n",
    "    L=len(parameters)//2\n",
    "    for l in range(1,L+1):\n",
    "        v[f\"W{l}\"]=beta*v[f\"W{l}\"]+(1-beta)*grads[f\"dW{l}\"]\n",
    "        v[f\"b{l}\"]=beta*v[f\"b{l}\"]+(1-beta)*grads[f\"db{l}\"]\n",
    "        parameters[f\"W{l}\"]-=learning_rate*v[f\"W{l}\"]\n",
    "        parameters[f\"b{l}\"]-=learning_rate*v[f\"b{l}\"]\n",
    "    return parameters, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_parameters_adam(parameters):\n",
    "    v={}\n",
    "    s={}\n",
    "    L=len(parameters)//2\n",
    "    for l in range(1,L+1):\n",
    "        v[f\"W{l}\"]=np.zeros((parameters[f\"W{l}\"].shape[0],parameters[f\"W{l}\"].shape[1]))\n",
    "        s[f\"W{l}\"]=np.zeros((parameters[f\"W{l}\"].shape[0],parameters[f\"W{l}\"].shape[1]))\n",
    "        v[f\"b{l}\"]=np.zeros((parameters[f\"b{l}\"].shape[0],1))\n",
    "        s[f\"b{l}\"]=np.zeros((parameters[f\"b{l}\"].shape[0],1))\n",
    "    return v,s\n",
    "    #initialize and return adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_adam(parameters, grads, learning_rate, v, s,t, beta1=0.9, beta2=0.999,epsilon_s=1e-8):\n",
    "    #using higher learning_rate(alpha) and less data is possible now!\n",
    "    L=len(parameters)//2\n",
    "    v_corrected={}\n",
    "    s_corrected={}\n",
    "    for l in range(1,L+1):\n",
    "        v[f\"W{l}\"]=beta1*v[f\"W{l}\"]+(1-beta1)*grads[f\"dW{l}\"]\n",
    "        v[f\"b{l}\"]=beta1*v[f\"b{l}\"]+(1-beta1)*grads[f\"db{l}\"]\n",
    "        v_corrected[f\"W{l}\"]=v[f\"W{l}\"]/(1-np.power(beta1,t))\n",
    "        v_corrected[f\"b{l}\"]=v[f\"b{l}\"]/(1-np.power(beta1,t))\n",
    "        s[f\"W{l}\"]=beta2*s[f\"W{l}\"]+(1-beta2)*np.power(grads[f\"dW{l}\"],2)\n",
    "        s[f\"b{l}\"]=beta2*s[f\"b{l}\"]+(1-beta2)*np.power(grads[f\"db{l}\"],2)\n",
    "        s_corrected[f\"W{l}\"]=s[f\"W{l}\"]/(1-np.power(beta2,t))\n",
    "        s_corrected[f\"b{l}\"]=s[f\"b{l}\"]/(1-np.power(beta2,t))\n",
    "        parameters[f\"W{l}\"]-=learning_rate*v_corrected[f\"W{l}\"]/(np.sqrt(s_corrected[f\"W{l}\"])+epsilon_s)\n",
    "        parameters[f\"b{l}\"]-=learning_rate*v_corrected[f\"b{l}\"]/(np.sqrt(s_corrected[f\"b{l}\"])+epsilon_s)\n",
    "    return parameters, v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionary_to_vector(parameters):\n",
    "    L=len(parameters)//2\n",
    "    parameterscount={}\n",
    "    for l in range(1,L+1):\n",
    "        wvector=np.reshape(parameters[f\"W{l}\"],(-1,1)) #create vector for every w\n",
    "        bvector=np.reshape(parameters[f\"b{l}\"],(-1,1)) #create vector for every b\n",
    "        if l==1:\n",
    "            vectors=wvector\n",
    "        else:\n",
    "            vectors=np.concatenate((vectors,wvector),axis=0)\n",
    "        vectors=np.concatenate((vectors,bvector),axis=0)\n",
    "        parameterscount[f\"W{l}\"]=wvector.shape[0]\n",
    "        parameterscount[f\"b{l}\"]=bvector.shape[0]\n",
    "    return vectors, parameterscount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graddict_to_vector(grad):\n",
    "    L=len(grad)//3\n",
    "    gradcount={}\n",
    "    for l in range(1,L+1):\n",
    "        dwvector=np.reshape(grad[f\"dW{l}\"],(-1,1)) #create vector for every w\n",
    "        dbvector=np.reshape(grad[f\"db{l}\"],(-1,1)) #create vector for every b\n",
    "        if l==1:\n",
    "            vectors=dwvector\n",
    "        else:\n",
    "            vectors=np.concatenate((vectors,dwvector),axis=0)\n",
    "        vectors=np.concatenate((vectors,dbvector),axis=0)\n",
    "        gradcount[f\"dW{l}\"]=dwvector.shape[0]\n",
    "        gradcount[f\"db{l}\"]=dbvector.shape[0]\n",
    "    return vectors, gradcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_to_dictionary(vectors,layers_dims,parameterscount):\n",
    "    L=len(layers_dims) #number of layer + 1\n",
    "    parameters={}\n",
    "    prevamount=0\n",
    "    currentamount=0\n",
    "    for l in range(1,L):\n",
    "        currentamount+=layers_dims[l-1]*layers_dims[l]\n",
    "        wvector=(vectors[prevamount:currentamount]).reshape((layers_dims[l],layers_dims[l-1]))\n",
    "        prevamount=currentamount\n",
    "        currentamount+=layers_dims[l]\n",
    "        bvector=(vectors[prevamount:currentamount]).reshape((layers_dims[l],1))\n",
    "        prevamount=currentamount\n",
    "        parameters[f\"W{l}\"]=wvector\n",
    "        parameters[f\"b{l}\"]=bvector\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_checking(X,Y,layers_dims,parameters_dict,grads_dict,epsilon,activation1,activation_final):\n",
    "    grads_vector,_=graddict_to_vector(grads_dict)\n",
    "    parameters_vector,parameterscount=dictionary_to_vector(parameters_dict)\n",
    "    parameters_members=parameters_vector.shape[0]\n",
    "    J_plus=np.zeros(grads_vector.shape)\n",
    "    J_minus=np.zeros(grads_vector.shape)\n",
    "    gradapprox=np.zeros(grads_vector.shape)\n",
    "    for i in range(parameters_members):\n",
    "        #compute every J(theta plus)\n",
    "        print(i)\n",
    "        theta_plus=np.copy(grads_vector)\n",
    "        theta_plus[i][0]+=epsilon\n",
    "        AL,_=forward_propagation_deep(X, vector_to_dictionary(parameters_vector,layers_dims,parameterscount), activation1, activation_final)\n",
    "        J_plus[i]=compute_cost(AL,Y)\n",
    "        #compute every J(theta minus)\n",
    "        theta_minus=np.copy(grads_vector)\n",
    "        theta_minus[i][0]+=epsilon\n",
    "        AL,_=forward_propagation_deep(X, vector_to_dictionary(parameters_vector,layers_dims,parameterscount), activation1, activation_final)\n",
    "        J_minus[i]=compute_cost(AL,Y)\n",
    "        #compute gradient approximation\n",
    "        gradapprox[i]=(J_plus[i]-J_minus[i])/(2*epsilon)\n",
    "    numerator=np.linalg.norm(grads_vector-gradapprox)\n",
    "    denominator=(np.linalg.norm(grads_vector))+np.linalg.norm(gradapprox)\n",
    "    difference=numerator-denominator\n",
    "    if difference<=2e-7:\n",
    "        print(\"Your gradient is correct! difference: \"+str(difference))\n",
    "    else:\n",
    "        print(\"There might be a mistake! difference: \"+str(difference))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X,Y,layers_dims,activation1=\"relu\",activation_final=\"sigmoid\",num_iteration=3000,learning_rate=0.075,printcost=False, init=\"he\", keep_prob=1.0, lambd=0,epsilon=0,epsilon_s=1e-8,grad_desc=\"adam\",beta1=0.9,beta2=0.999):\n",
    "    \"\"\"X is the data that will be used as a training set (data features, datapoints)\n",
    "    Y is the target data in shape of (1,amount of datapoints)\n",
    "    layers is a list that contain the number of nodes in each layer,\n",
    "    activation1 will be used for layer 1 - layer L-1\n",
    "    algorithm_final will be used for layer L before output\n",
    "    num_iteration is the amount of iteration of training\n",
    "    learning_rate is the amount that will be used for updating the parameter\n",
    "    printcost will printcost every 100 iteration\"\"\"\n",
    "    if init==\"he\":\n",
    "        parameters=init_parameters_deep_he(layers_dims)\n",
    "    else:\n",
    "        parameters=init_parameters_deep(layers_dims)\n",
    "    costs=[]\n",
    "    layer=len(layers_dims)#number of hidden layer until L\n",
    "    if grad_desc==\"momentum\":\n",
    "        v=init_parameters_momentum(parameters)\n",
    "    elif grad_desc==\"adam\":\n",
    "        v,s=init_parameters_adam(parameters)\n",
    "        t=1\n",
    "    if lambd==0 and keep_prob==1:\n",
    "        for i in range (0,num_iteration):\n",
    "            AL,caches=forward_propagation_deep(X, parameters, activation1, activation_final)\n",
    "            cost=compute_cost(AL,Y)\n",
    "            grads=back_propagation_deep(AL,Y,caches,activation1,activation_final)\n",
    "            if grad_desc==\"normal\":\n",
    "                #normal gradient descent\n",
    "                parameters=update_parameters_graddesc(parameters,grads,learning_rate)\n",
    "            elif grad_desc==\"momentum\":\n",
    "                #momentum gradient descent\n",
    "                parameters, v= update_parameters_momentum(parameters, grads, learning_rate, v, beta1)\n",
    "            elif grad_desc==\"adam\":\n",
    "                #adam gradient descent\n",
    "                parameters, v, s= update_parameters_adam(parameters, grads, learning_rate, v, s, t, beta1, beta2,epsilon_s)\n",
    "                t+=1\n",
    "            else:\n",
    "                print(\"Error in gradient descent mode\")\n",
    "                raise ValueError\n",
    "            if printcost and i%100==0:\n",
    "                print(f\"Cost after {i} iteration: {cost}\")\n",
    "                costs.append(cost)\n",
    "            if epsilon!=0 and i%1000==0:\n",
    "                gradient_checking(X,Y,layers_dims,parameters,grads,epsilon,activation1,activation_final)\n",
    "    elif lambd!=0:\n",
    "        for i in range(0, num_iteration):\n",
    "            AL,caches=forward_propagation_deep(X,parameters,activation1,activation_final)\n",
    "            cost=compute_cost_l2(AL, Y, lambd,parameters,layers_dims)\n",
    "            grads=back_propagation_deep_l2(AL, Y, caches, activation1,activation_final,lambd,parameters)\n",
    "            if grad_desc==\"normal\":\n",
    "                #normal gradient descent\n",
    "                parameters=update_parameters_graddesc(parameters,grads,learning_rate)\n",
    "            elif grad_desc==\"momentum\":\n",
    "                #momentum gradient descent\n",
    "                parameters, v= update_parameters_momentum(parameters, grads, learning_rate, v, beta1)\n",
    "            elif grad_desc==\"adam\":\n",
    "                #adam gradient descent\n",
    "                parameters, v, s= update_parameters_adam(parameters, grads, learning_rate, v, s, t, beta1, beta2,epsilon_s)\n",
    "                t+=1\n",
    "            else:\n",
    "                print(\"Error in gradient descent mode\")\n",
    "                raise ValueError\n",
    "            if printcost and i%100==0:\n",
    "                print(f\"Cost after {i} iteration: {cost}\")\n",
    "                costs.append(cost)\n",
    "            if epsilon!=0 and i%1000==0:\n",
    "                gradient_checking(X,Y,layers_dims,parameters,grads,epsilon,activation1,activation_final)\n",
    "    elif keep_prob<1.0:\n",
    "        for i in range(0, num_iteration):\n",
    "            AL,caches, D_collection=forward_propagation_deep_dropout(X,parameters,activation1,activation_final,keep_prob)\n",
    "            cost=compute_cost(AL, Y)\n",
    "            grads=back_propagation_deep_dropout(AL, Y, caches, D_collection, activation1,activation_final,keep_prob)\n",
    "            if grad_desc==\"normal\":\n",
    "                #normal gradient descent\n",
    "                parameters=update_parameters_graddesc(parameters,grads,learning_rate)\n",
    "            elif grad_desc==\"momentum\":\n",
    "                #momentum gradient descent\n",
    "                parameters, v= update_parameters_momentum(parameters, grads, learning_rate, v, beta1)\n",
    "            elif grad_desc==\"adam\":\n",
    "                #adam gradient descent\n",
    "                parameters, v, s= update_parameters_adam(parameters, grads, learning_rate, v, s, t, beta1, beta2,epsilon_s)\n",
    "                t+=1\n",
    "            else:\n",
    "                print(\"Error in gradient descent mode\")\n",
    "                raise ValueError\n",
    "            if printcost and i%100==0:\n",
    "                print(f\"Cost after {i} iteration: {cost}\")\n",
    "                costs.append(cost)\n",
    "            if epsilon!=0 and i%1000==0:\n",
    "                gradient_checking(X,Y,layers_dims,parameters,grads,epsilon,activation1,activation_final)\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial=np.array([1.1,0,0.9])\n",
    "print(relu(trial))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(X,Y,parameters,activation1,activation_final):\n",
    "    \"\"\"input:\n",
    "    parameters= a dictionary of parameters that have been trained previously\n",
    "    layerdims=a list containing the amount of nodes in each layer\n",
    "    result= matrix containing the target value\n",
    "    activation1=the activation function for layer 1 - layer L-1\n",
    "    activation2=the activation function for layer L\n",
    "    \n",
    "    \n",
    "    Output:\n",
    "    accuracy=the accuracy of the output that is given by the model\"\"\"\n",
    "    AL,caches=forward_propagation_deep(X, parameters, activation1, activation_final)\n",
    "    m=int(AL.shape[1])\n",
    "    output=np.zeros((1,m))\n",
    "    \n",
    "    for i in range(0,m):\n",
    "        if AL[0,i]>0.5:\n",
    "            output[0,i]=1\n",
    "        else:\n",
    "            output[0,i]=0\n",
    "    accuracy=np.sum((output==Y)/m)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters,activation1,activation_final):\n",
    "    print(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

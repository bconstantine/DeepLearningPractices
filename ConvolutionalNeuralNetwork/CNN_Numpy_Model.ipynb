{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#the following codes are copy pasted from the Andrew Ng CNN Model step by step program\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "padding\n",
    "1. For Making the output size not shrink, so the result may still be in the same shape. Useful if we implement deep layers\n",
    "2. Keeps the information at the border of the image, since they are still evaluated\n",
    "\n",
    "For padding, we can use np.pad(a, (pad shape for every layer), mode = \"constant\", constant = (0,0))\n",
    "what this means is we pad the numpy array a with a decided pad shape for every layer, include every layer with\n",
    "a tuple, and inside the tuple insert two number, which indicate the before n and after n. If we set the mode to constant,\n",
    "we set the value of the pad as constant, we can modify the value inside constant_values=. Personally, we can modify our own\n",
    "constant_valuees in each layer, but if we set it only 1 tuple, this standard will follow the rest. The first number is for the before n\n",
    "value, the second one is for the after n value. By default, mode is set to constant and constant_values = 0.\n",
    "\"\"\"\n",
    "\n",
    "def zeroPad(x, nPad):\n",
    "    \"\"\"Pad numpy array x with 0 in before and beginning based on the nPad number. For now, we want the horizontal and vertical\n",
    "    pad to be the same number\n",
    "    \n",
    "    input:\n",
    "    x = np.array in the shape of [m, nh, nw, nc]\n",
    "    nPad = integer that symbolizes the number of pad for the layer  nh and nw, should be bigger than 0\n",
    "    \n",
    "    return:\n",
    "    paddedX = np.array in the shape of [m, nh + 2*nPad, nw + 2*nPad, nc]. All padded by zero\"\"\"\n",
    "    \n",
    "    paddedX = np.pad(x, ((0,0), (nPad,nPad), (nPad,nPad), (0,0)), mode = \"constant\", constant_values = (0,0))\n",
    "    return paddedX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape =\n",
      " (4, 3, 3, 2)\n",
      "x_pad.shape =\n",
      " (4, 7, 7, 2)\n",
      "x[1,1] =\n",
      " [[ 0.90085595 -0.68372786]\n",
      " [-0.12289023 -0.93576943]\n",
      " [-0.26788808  0.53035547]]\n",
      "x_pad[1,1] =\n",
      " [[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2e51ead6988>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#this test is copied from Andrew Ng test code\n",
    "np.random.seed(1)\n",
    "x = np.random.randn(4, 3, 3, 2)\n",
    "x_pad = zeroPad(x, 2)\n",
    "print (\"x.shape =\\n\", x.shape)\n",
    "print (\"x_pad.shape =\\n\", x_pad.shape)\n",
    "print (\"x[1,1] =\\n\", x[1,1])\n",
    "print (\"x_pad[1,1] =\\n\", x_pad[1,1])\n",
    "\n",
    "#subplots is used to make a 'drawing board' for displaying the picture\n",
    "fig, axarr = plt.subplots(1, 2) #the figure has 1 row and 2 column (hence 2 picture)\n",
    "#fig is a class inside matplotlib, while axarr is the array of axis to plot\n",
    "axarr[0].set_title('x')\n",
    "axarr[0].imshow(x[0,:,:,0])\n",
    "axarr[1].set_title('x_pad')\n",
    "axarr[1].imshow(x_pad[0,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Implement the calculation of sliced a with the convolution filter and bias by:\n",
    "1. Element wise multiplication of sliced a with the convolution filter (np.multiply)\n",
    "2. sum the result numpy array (np.sum)\n",
    "3. add the bias\"\"\"\n",
    "\n",
    "def singleStepConvolution(aSlice, w, b):\n",
    "    \"\"\"element multiplication for aSlice with filtArray, sum the result of the multiplication, and add the bias\n",
    "    just as in 1 step of convolution\n",
    "    \n",
    "    Parameters:\n",
    "    - aSlice = aLayer that has been crop so that it has the same shape with w, shape = [f, f, nc]\n",
    "    - w = np.array for the filter [f, f, nc]\n",
    "    - b = np.array of bias that wants to be added after the sum, shape = (1,1)\n",
    "    \n",
    "    Return:\n",
    "    - res = ffloat of the convolution result of the previous part\"\"\"\n",
    "    \n",
    "    elementMultiplied = np.multiply(aSlice, w)\n",
    "    summedElement = np.sum(elementMultiplied)\n",
    "    summedElement+=float(b)\n",
    "    return summedElement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = -6.999089450680221\n"
     ]
    }
   ],
   "source": [
    "#this test case is copy pasted from the Andrew Ng\n",
    "np.random.seed(1)\n",
    "a_slice_prev = np.random.randn(4, 4, 3)\n",
    "W = np.random.randn(4, 4, 3)\n",
    "b = np.random.randn(1, 1, 1)\n",
    "\n",
    "Z = singleStepConvolution(a_slice_prev, W, b)\n",
    "print(\"Z =\", Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convForward(A_prev, w, b, hparameters):\n",
    "    \"\"\"This function is used to implement one layer of convolution process\n",
    "    parameters:\n",
    "    A_prev = a numpy array in a shape of (m, nH_prev, nW_prev, nC_prev)\n",
    "    w = the filter for the convolution, in a shape of (f, f, nC_prev, nC)\n",
    "    b = the bias for each convolution process, in a shape of (1,1,1,nC)\n",
    "    hparameters = the dictionary containing 'stride' and 'pad'\n",
    "    \n",
    "    return value:\n",
    "    Z = result of the convolution in shape of (m, nH, nW, nC)\n",
    "    cache = value needed for backward prop of convolution, which is the A_prev, w, b, hparameters\"\"\"\n",
    "    \n",
    "    m, nH_prev, nW_prev, nC_prev = A_prev.shape[0], A_prev.shape[1], A_prev.shape[2], A_prev.shape[3]\n",
    "    \n",
    "    f, nC = w.shape[0], w.shape[3]\n",
    "    \n",
    "    stride = hparameters[\"stride\"]\n",
    "    pad = hparameters[\"pad\"]\n",
    "    \n",
    "    paddedA_prev = zeroPad(A_prev, pad)\n",
    "    \n",
    "    #start doing the convolution using for\n",
    "    nH = int((nH_prev + 2*pad - f)/stride) + 1\n",
    "    nW = int((nW_prev + 2*pad - f)/stride) + 1\n",
    "    \n",
    "    #initialize the output\n",
    "    z = np.zeros((m, nH, nW, nC))\n",
    "    \n",
    "    #starting the for\n",
    "    for datapoint in range(m):\n",
    "        for vertical in range(nH):\n",
    "            verticalStart = vertical * stride\n",
    "            verticalEnd = verticalStart + f #the end index will not join\n",
    "            for horizontal in range(nW):\n",
    "                horizontalStart = horizontal * stride\n",
    "                horizontalEnd = horizontalStart + f #the end index will not join\n",
    "                for curChannel in range(nC):\n",
    "                    slicedA_prev = paddedA_prev[datapoint, verticalStart:verticalEnd, horizontalStart:horizontalEnd, :]\n",
    "                    currentFilter = w[:,:,:,curChannel]\n",
    "                    currentBias = b[:,:,:,curChannel]\n",
    "                    convResult = singleStepConvolution(slicedA_prev, currentFilter, currentBias)\n",
    "                    z[datapoint, vertical, horizontal, curChannel] = convResult\n",
    "    # Making sure your output shape is correct\n",
    "    assert(z.shape == (m, nH, nW, nC))\n",
    "    #everything has been set, starting making the cache and returning the z, cache\n",
    "    cache = A_prev, w, b, hparameters\n",
    "    return z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z's mean =\n",
      " 0.6923608807576933\n",
      "Z[3,2,1] =\n",
      " [-1.28912231  2.27650251  6.61941931  0.95527176  8.25132576  2.31329639\n",
      " 13.00689405  2.34576051]\n",
      "cache_conv[0][1][2][3] =\n",
      " [-1.1191154   1.9560789  -0.3264995  -1.34267579]\n"
     ]
    }
   ],
   "source": [
    "#This test case is from the Andrew Ng Exercise\n",
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(10,5,7,4)\n",
    "W = np.random.randn(3,3,4,8)\n",
    "b = np.random.randn(1,1,1,8)\n",
    "hparameters = {\"pad\" : 1,\n",
    "               \"stride\": 2}\n",
    "\n",
    "Z, cache_conv = convForward(A_prev, W, b, hparameters)\n",
    "print(\"Z's mean =\\n\", np.mean(Z))\n",
    "print(\"Z[3,2,1] =\\n\", Z[3,2,1])\n",
    "print(\"cache_conv[0][1][2][3] =\\n\", cache_conv[0][1][2][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poolForward(A_prev, hparameters, mode = \"max\"):\n",
    "    \"\"\"This function is used as a pooling, where it works like convolution but no parameters\n",
    "    needed because we can directly count the average or the max value of the sliced ones\n",
    "    \n",
    "    Parameters:\n",
    "    A_prev: numpy array in the shape of (m, nHPrev, nWPrev, nC)\n",
    "    hparameters: a dictionary containing 'f' (filter size) and 'stride', no pad\n",
    "    mode  : choose between 'average' or 'max'\n",
    "    \n",
    "    return:\n",
    "    A : numpy array output of thee pooling, in shape of (m, nH, nW, nC)\n",
    "    cache: tuple containing input, containing A_prev and hparameters\"\"\"\n",
    "    \n",
    "    #retrieve the m, nHPrev, nWPrev, nC\n",
    "    m, nHPrev, nWPrev, nC = A_prev.shape[0], A_prev.shape[1], A_prev.shape[2], A_prev.shape[3]\n",
    "    f, stride = hparameters[\"f\"], hparameters[\"stride\"]\n",
    "    \n",
    "    #using the int to floor the result\n",
    "    nH = int((nHPrev - f)/stride) + 1\n",
    "    nW = int((nWPrev - f)/stride) + 1\n",
    "    \n",
    "    #make an empty Z\n",
    "    A = np.zeros((m, nH, nW, nC))\n",
    "    for datapoint in range(m):\n",
    "        for vertical in range(nH):\n",
    "            verticalStart = vertical*stride\n",
    "            verticalEnd = verticalStart + f\n",
    "            for horizontal in range(nW):\n",
    "                horizontalStart = horizontal*stride\n",
    "                horizontalEnd = horizontalStart + f\n",
    "                for currentChannel in range(nC):\n",
    "                    ASliced = A_prev[datapoint, verticalStart:verticalEnd, horizontalStart:horizontalEnd, currentChannel]\n",
    "                    if(mode == \"max\"):\n",
    "                        A[datapoint, vertical, horizontal, currentChannel] = np.max(ASliced)\n",
    "                    elif(mode == \"average\"):\n",
    "                        A[datapoint, vertical, horizontal, currentChannel] = np.mean(ASliced)\n",
    "                    else:\n",
    "                        raise ValueError\n",
    "    # Making sure your output shape is correct\n",
    "    assert(A.shape == (m, nH, nW, nC))\n",
    "    cache = (A_prev, hparameters)\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode = max\n",
      "A.shape = (2, 3, 3, 3)\n",
      "A =\n",
      " [[[[1.74481176 0.90159072 1.65980218]\n",
      "   [1.74481176 1.46210794 1.65980218]\n",
      "   [1.74481176 1.6924546  1.65980218]]\n",
      "\n",
      "  [[1.14472371 0.90159072 2.10025514]\n",
      "   [1.14472371 0.90159072 1.65980218]\n",
      "   [1.14472371 1.6924546  1.65980218]]\n",
      "\n",
      "  [[1.13162939 1.51981682 2.18557541]\n",
      "   [1.13162939 1.51981682 2.18557541]\n",
      "   [1.13162939 1.6924546  2.18557541]]]\n",
      "\n",
      "\n",
      " [[[1.19891788 0.84616065 0.82797464]\n",
      "   [0.69803203 0.84616065 1.2245077 ]\n",
      "   [0.69803203 1.12141771 1.2245077 ]]\n",
      "\n",
      "  [[1.96710175 0.84616065 1.27375593]\n",
      "   [1.96710175 0.84616065 1.23616403]\n",
      "   [1.62765075 1.12141771 1.2245077 ]]\n",
      "\n",
      "  [[1.96710175 0.86888616 1.27375593]\n",
      "   [1.96710175 0.86888616 1.23616403]\n",
      "   [1.62765075 1.12141771 0.79280687]]]]\n",
      "\n",
      "mode = average\n",
      "A.shape = (2, 3, 3, 3)\n",
      "A =\n",
      " [[[[-3.01046719e-02 -3.24021315e-03 -3.36298859e-01]\n",
      "   [ 1.43310483e-01  1.93146751e-01 -4.44905196e-01]\n",
      "   [ 1.28934436e-01  2.22428468e-01  1.25067597e-01]]\n",
      "\n",
      "  [[-3.81801899e-01  1.59993515e-02  1.70562706e-01]\n",
      "   [ 4.73707165e-02  2.59244658e-02  9.20338402e-02]\n",
      "   [ 3.97048605e-02  1.57189094e-01  3.45302489e-01]]\n",
      "\n",
      "  [[-3.82680519e-01  2.32579951e-01  6.25997903e-01]\n",
      "   [-2.47157416e-01 -3.48524998e-04  3.50539717e-01]\n",
      "   [-9.52551510e-02  2.68511000e-01  4.66056368e-01]]]\n",
      "\n",
      "\n",
      " [[[-1.73134159e-01  3.23771981e-01 -3.43175716e-01]\n",
      "   [ 3.80634669e-02  7.26706274e-02 -2.30268958e-01]\n",
      "   [ 2.03009393e-02  1.41414785e-01 -1.23158476e-02]]\n",
      "\n",
      "  [[ 4.44976963e-01 -2.61694592e-03 -3.10403073e-01]\n",
      "   [ 5.08114737e-01 -2.34937338e-01 -2.39611830e-01]\n",
      "   [ 1.18726772e-01  1.72552294e-01 -2.21121966e-01]]\n",
      "\n",
      "  [[ 4.29449255e-01  8.44699612e-02 -2.72909051e-01]\n",
      "   [ 6.76351685e-01 -1.20138225e-01 -2.44076712e-01]\n",
      "   [ 1.50774518e-01  2.89111751e-01  1.23238536e-03]]]]\n"
     ]
    }
   ],
   "source": [
    "#test case from Andrew Ng Course\n",
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(2, 5, 5, 3)\n",
    "hparameters = {\"stride\" : 1, \"f\": 3}\n",
    "\n",
    "A, cache = poolForward(A_prev, hparameters)\n",
    "print(\"mode = max\")\n",
    "print(\"A.shape = \" + str(A.shape))\n",
    "print(\"A =\\n\", A)\n",
    "print()\n",
    "A, cache = poolForward(A_prev, hparameters, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print(\"A.shape = \" + str(A.shape))\n",
    "print(\"A =\\n\", A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode = max\n",
      "A.shape = (2, 2, 2, 3)\n",
      "A =\n",
      " [[[[1.74481176 0.90159072 1.65980218]\n",
      "   [1.74481176 1.6924546  1.65980218]]\n",
      "\n",
      "  [[1.13162939 1.51981682 2.18557541]\n",
      "   [1.13162939 1.6924546  2.18557541]]]\n",
      "\n",
      "\n",
      " [[[1.19891788 0.84616065 0.82797464]\n",
      "   [0.69803203 1.12141771 1.2245077 ]]\n",
      "\n",
      "  [[1.96710175 0.86888616 1.27375593]\n",
      "   [1.62765075 1.12141771 0.79280687]]]]\n",
      "\n",
      "mode = average\n",
      "A.shape = (2, 2, 2, 3)\n",
      "A =\n",
      " [[[[-0.03010467 -0.00324021 -0.33629886]\n",
      "   [ 0.12893444  0.22242847  0.1250676 ]]\n",
      "\n",
      "  [[-0.38268052  0.23257995  0.6259979 ]\n",
      "   [-0.09525515  0.268511    0.46605637]]]\n",
      "\n",
      "\n",
      " [[[-0.17313416  0.32377198 -0.34317572]\n",
      "   [ 0.02030094  0.14141479 -0.01231585]]\n",
      "\n",
      "  [[ 0.42944926  0.08446996 -0.27290905]\n",
      "   [ 0.15077452  0.28911175  0.00123239]]]]\n"
     ]
    }
   ],
   "source": [
    "# Case 2: stride of 2\n",
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(2, 5, 5, 3)\n",
    "hparameters = {\"stride\" : 2, \"f\": 3}\n",
    "\n",
    "A, cache = poolForward(A_prev, hparameters)\n",
    "print(\"mode = max\")\n",
    "print(\"A.shape = \" + str(A.shape))\n",
    "print(\"A =\\n\", A)\n",
    "print()\n",
    "\n",
    "A, cache = poolForward(A_prev, hparameters, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print(\"A.shape = \" + str(A.shape))\n",
    "print(\"A =\\n\", A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FROM ANDREW NG COURSE\n",
    "## 5 - Backpropagation in convolutional neural networks \n",
    "### 5.1 - Convolutional layer backward pass \n",
    "\n",
    "Let's start by implementing the backward pass for a CONV layer. \n",
    "\n",
    "#### 5.1.1 - Computing dA:\n",
    "This is the formula for computing $dA$ with respect to the cost for a certain filter $W_c$ and a given training example:\n",
    "\n",
    "$$ dA += \\sum _{h=0} ^{n_H} \\sum_{w=0} ^{n_W} W_c \\times dZ_{hw} \\tag{1}$$\n",
    "\n",
    "Where $W_c$ is a filter and $dZ_{hw}$ is a scalar corresponding to the gradient of the cost with respect to the output of the conv layer Z at the hth row and wth column (corresponding to the dot product taken at the ith stride left and jth stride down). Note that at each time, we multiply the the same filter $W_c$ by a different dZ when updating dA. We do so mainly because when computing the forward propagation, each filter is dotted and summed by a different a_slice. Therefore when computing the backprop for dA, we are just adding the gradients of all the a_slices. \n",
    "\n",
    "In code, inside the appropriate for-loops, this formula translates into:\n",
    "```python\n",
    "da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
    "```\n",
    "\n",
    "#### 5.1.2 - Computing dW:\n",
    "This is the formula for computing $dW_c$ ($dW_c$ is the derivative of one filter) with respect to the loss:\n",
    "\n",
    "$$ dW_c  += \\sum _{h=0} ^{n_H} \\sum_{w=0} ^ {n_W} a_{slice} \\times dZ_{hw}  \\tag{2}$$\n",
    "\n",
    "Where $a_{slice}$ corresponds to the slice which was used to generate the activation $Z_{ij}$. Hence, this ends up giving us the gradient for $W$ with respect to that slice. Since it is the same $W$, we will just add up all such gradients to get $dW$. \n",
    "\n",
    "In code, inside the appropriate for-loops, this formula translates into:\n",
    "```python\n",
    "dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
    "```\n",
    "\n",
    "#### 5.1.3 - Computing db:\n",
    "\n",
    "This is the formula for computing $db$ with respect to the cost for a certain filter $W_c$:\n",
    "\n",
    "$$ db = \\sum_h \\sum_w dZ_{hw} \\tag{3}$$\n",
    "\n",
    "As you have previously seen in basic neural networks, db is computed by summing $dZ$. In this case, you are just summing over all the gradients of the conv output (Z) with respect to the cost. \n",
    "\n",
    "In code, inside the appropriate for-loops, this formula translates into:\n",
    "```python\n",
    "db[:,:,:,c] += dZ[i, h, w, c]\n",
    "```\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convBackward(dZ, cache):\n",
    "    \"\"\"This function is used as a function to find back propagation of Convolution layer,\n",
    "    mathematical formula is pasted on the previous cell\n",
    "    \n",
    "    Input:\n",
    "    dZ: partial derivative of loss of dZ(shape = m, nH, nW, nC) from next cell\n",
    "    cache: convForward cache, containing APrev (shape = m, nHPrev, nWPrev, nCprev), \n",
    "    w (shape = f,f,nCPrev,nC), b (shape = 1x1x1xnC), hparameters(containing int stride and pad)\n",
    "    \n",
    "    \n",
    "    Output:\n",
    "    dAPrev = derivative of dL/dAPrev, with shape = APrev.shape\n",
    "    dW = derivative of dL/dW, with shape = w.shape\n",
    "    db = derivative of db/dw, with shape = b.shape\"\"\"\n",
    "    \n",
    "    \n",
    "    #retrieve every important shape data\n",
    "    m, nH, nW, nC = dZ.shape[0], dZ.shape[1], dZ.shape[2], dZ.shape[3]\n",
    "    APrev, w, b, hparameters = cache[0], cache[1], cache[2], cache[3]\n",
    "    nHPrev, nWPrev, nCPrev = APrev.shape[1], APrev.shape[2], APrev.shape[3]\n",
    "    f = w.shape[0]\n",
    "    stride = hparameters[\"stride\"]\n",
    "    pad = hparameters[\"pad\"]\n",
    "    \n",
    "    #initialize dAPrev, dw, db\n",
    "    dAPrev = np.zeros((APrev.shape))\n",
    "    dW = np.zeros((w.shape))\n",
    "    db = np.zeros((b.shape))\n",
    "    \n",
    "    #pad the APrev, because it hasnt padded like the one inside the forward prop\n",
    "    APrevPad = zeroPad(APrev, pad)\n",
    "    dAPrevPad = zeroPad(dAPrev, pad)\n",
    "    \n",
    "    #iterate for datapoint\n",
    "    for datapoint in range(m):\n",
    "        aPrevPad = APrevPad[datapoint, :,:,:]\n",
    "        daPrevPad = dAPrevPad[datapoint, :, :,:]\n",
    "        #iterate for vertical (output based)\n",
    "        for vertical in range(nH):\n",
    "            #decide the start for the input\n",
    "            vertStart = vertical*stride\n",
    "            vertEnd = vertStart + f\n",
    "            #iterate for horizontal (output based)\n",
    "            for horizontal in range(nW):\n",
    "                #decide the start for the input\n",
    "                horiStart = horizontal * stride\n",
    "                horiEnd = horiStart + f\n",
    "                #iterate for output channel\n",
    "                for channel in range(nC):\n",
    "                    #cut aSlice based on the vertStart, vertEnd and horiStart, horiEnd of aPrevPad\n",
    "                    aSlicedPad = aPrevPad[vertStart:vertEnd, horiStart:horiEnd, :]  #nCPrev use all because they all contribute to the layer\n",
    "                    \n",
    "                    #start counting\n",
    "                    daPrevPad[vertStart:vertEnd, horiStart:horiEnd, :] += w[:,:,:,channel] * dZ[datapoint,vertical,horizontal,channel]\n",
    "                    dW[:,:,:,channel] += dZ[datapoint,vertical,horizontal,channel] * aSlicedPad #it affects one whole layer because this filter is used for one output channel\n",
    "                    db[:,:,:,channel] += dZ[datapoint,vertical,horizontal,channel]\n",
    "        #set back each datapoint  to its respective dAPrev\n",
    "        dAPrev[datapoint,:,:,:] = daPrevPad[pad:-pad,pad:-pad,:]\n",
    "        \n",
    "    # Making sure your output shape is correct\n",
    "    assert(dAPrev.shape == (m, nHPrev, nWPrev, nCPrev))\n",
    "    \n",
    "    return dAPrev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_mean = 1.4524377775388075\n",
      "dW_mean = 1.7269914583139097\n",
      "db_mean = 7.839232564616838\n"
     ]
    }
   ],
   "source": [
    "#this test case is from ANDREW NG\n",
    "# We'll run conv_forward to initialize the 'Z' and 'cache_conv\",\n",
    "# which we'll use to test the conv_backward function\n",
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(10,4,4,3)\n",
    "W = np.random.randn(2,2,3,8)\n",
    "b = np.random.randn(1,1,1,8)\n",
    "hparameters = {\"pad\" : 2,\n",
    "               \"stride\": 2}\n",
    "Z, cache_conv = convForward(A_prev, W, b, hparameters)\n",
    "\n",
    "# Test conv_backward\n",
    "dA, dW, db = convBackward(Z, cache_conv)\n",
    "print(\"dA_mean =\", np.mean(dA))\n",
    "print(\"dW_mean =\", np.mean(dW))\n",
    "print(\"db_mean =\", np.mean(db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createMaskFromWindow(x):\n",
    "    \"\"\"Make a numpy array mask to identify where the max value of X\n",
    "    \n",
    "    Input:\n",
    "    x = np.array of shape(f,f)\n",
    "    \n",
    "    Output:\n",
    "    mask = array the same shape as x, containing True if the at the position where max is\"\"\"\n",
    "    \n",
    "    #true if x is max, false if x is not\n",
    "    mask = (x == np.max(x))\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  [[ 1.62434536 -0.61175641 -0.52817175]\n",
      " [-1.07296862  0.86540763 -2.3015387 ]]\n",
      "mask =  [[ True False False]\n",
      " [False False False]]\n"
     ]
    }
   ],
   "source": [
    "#This test case is from Andrew Ng\n",
    "np.random.seed(1)\n",
    "x = np.random.randn(2,3)\n",
    "mask = createMaskFromWindow(x)\n",
    "print('x = ', x)\n",
    "print(\"mask = \", mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distributeValue(dZ, shape):\n",
    "    \"\"\"This function is used if we want to equally distribute the value of dZ to each array\n",
    "    element in an array shape. \n",
    "    \n",
    "    Input:\n",
    "    dZ = scalar for dZ value\n",
    "    shape = (nH, nW) for the desired shape\n",
    "    \n",
    "    Output:\n",
    "    A = array of shape(nH, nW) after value of dZ is distributed\"\"\"\n",
    "    \n",
    "    A = np.ones(shape)\n",
    "    A *= (dZ / (shape[0] * shape[1]))\n",
    "    return A\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distributed value = [[0.5 0.5]\n",
      " [0.5 0.5]]\n"
     ]
    }
   ],
   "source": [
    "#This test case is from the Andrew Ng exercise\n",
    "a = distributeValue(2, (2,2))\n",
    "print('distributed value =', a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Putting everything together for pool backward\n",
    "def poolBackward(dA, cache, mode = \"max\"):\n",
    "    \"\"\"This function will be used to count the backward pass of the pooling layer\n",
    "    \n",
    "    Input:\n",
    "    dA = dL/dA until layer A (next layer after the pool)\n",
    "    cache = cache during the poolForward, containing APrev and hparameters \n",
    "    mode = the same mode used in poolForward, it can be max or average\n",
    "    \n",
    "    Output: \n",
    "    dAPrev: the dL/dAPrev of the previous input, same shape with APrev\"\"\"\n",
    "    \n",
    "    m, nH, nW, nC = dA.shape[0], dA.shape[1], dA.shape[2], dA.shape[3]\n",
    "    APrev, hparameters = cache\n",
    "    stride = hparameters[\"stride\"]\n",
    "    f = hparameters[\"f\"]\n",
    "    nWPrev, nBPrev = APrev.shape[1], APrev.shape[2] #no need to count nCPrev anymore since they are the same\n",
    "    \n",
    "    #intialize dAPrev\n",
    "    dAPrev = np.zeros(APrev.shape)\n",
    "    \n",
    "    for datapoint in range(m):\n",
    "        #evaluate based on output layer\n",
    "        for vertical in range(nH): \n",
    "            vertStart = vertical*stride\n",
    "            vertEnd = vertStart + f\n",
    "            for horizontal in range(nW):\n",
    "                horiStart = horizontal * stride\n",
    "                horiEnd = horiStart + f\n",
    "                for channel in range(nC):\n",
    "                    if mode == \"max\":\n",
    "                        aSlice = APrev[datapoint, vertStart:vertEnd, horiStart:horiEnd, channel] #retrieve the sliced A to check for max\n",
    "                    \n",
    "                        #check for max and create mask to multiply it\n",
    "                        mask = createMaskFromWindow(aSlice)\n",
    "                        \n",
    "                        #edit only the dAPrev with the max, by using += with (mask * dZ, so that only the one with true will be editted)\n",
    "                        dAPrev[datapoint, vertStart:vertEnd, horiStart:horiEnd, channel] += (mask * dA[datapoint, vertical, horizontal, channel])\n",
    "                    elif mode == \"average\":\n",
    "                        aSlice = APrev[datapoint, vertStart:vertEnd, horiStart:horiEnd, channel]\n",
    "                        \n",
    "                        #add the desired dAPrev with the distributeValue\n",
    "                        dAPrev[datapoint, vertStart:vertEnd, horiStart:horiEnd, channel] += distributeValue(dA[datapoint, vertical, horizontal, channel], [f,f])\n",
    "                    else:\n",
    "                        print(\"Undefined mode!\")\n",
    "                        raise ValueError\n",
    "                        \n",
    "    return dAPrev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode = max\n",
      "mean of dA =  0.14571390272918056\n",
      "dA_prev[1,1] =  [[ 0.          0.        ]\n",
      " [ 5.05844394 -1.68282702]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "mode = average\n",
      "mean of dA =  0.14571390272918056\n",
      "dA_prev[1,1] =  [[ 0.08485462  0.2787552 ]\n",
      " [ 1.26461098 -0.25749373]\n",
      " [ 1.17975636 -0.53624893]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(5, 5, 3, 2)\n",
    "hparameters = {\"stride\" : 1, \"f\": 2}\n",
    "A, cache = poolForward(A_prev, hparameters)\n",
    "dA = np.random.randn(5, 4, 2, 2)\n",
    "\n",
    "dA_prev = poolBackward(dA, cache, mode = \"max\")\n",
    "print(\"mode = max\")\n",
    "print('mean of dA = ', np.mean(dA))\n",
    "print('dA_prev[1,1] = ', dA_prev[1,1])  \n",
    "print()\n",
    "dA_prev = poolBackward(dA, cache, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print('mean of dA = ', np.mean(dA))\n",
    "print('dA_prev[1,1] = ', dA_prev[1,1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]\n",
      " [0. 1. 1.]\n",
      " [0. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "a = np.zeros((3,3))\n",
    "b = np.ones((2,2))\n",
    "a[1:3, 1:3] = b\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

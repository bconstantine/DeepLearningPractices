{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\Hp\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Hp\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Hp\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Hp\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Hp\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Hp\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\Hp\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Hp\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Hp\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Hp\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Hp\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Hp\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#This part of code is copied from Andrew Ng Face Recognition\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D, AveragePooling2D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.core import Lambda, Flatten, Dense\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "K.set_image_data_format('channels_first') #from now on every operation on keras will use [m,nc,nh,nw]\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import h5py\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#the autoreload will make sure that the imported module will always be reloaded\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize) #This way no longer numpy array will be summarized, everything will be printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_FLOATX = 'float32'\n",
    "\n",
    "\n",
    "\"\"\"Define the dictionary key for the weights of the downloaded FaceNet. Naming rules are as follows:\n",
    "    - If inception block is implemented, it will use 'inception' word in front of it\n",
    "    - inception block will specify the size of the filter (or pool for maxpool)\n",
    "    - when multiple conv or bn is implemented, it will give a number after the conv or bn (starting from 1)\n",
    "    - the closing dense layer will have the name 'dense_layer'\n",
    "\"\"\"\n",
    "WEIGHTS = [\n",
    "  'conv1', 'bn1', 'conv2', 'bn2', 'conv3', 'bn3',\n",
    "  'inception_3a_1x1_conv', 'inception_3a_1x1_bn',\n",
    "  'inception_3a_pool_conv', 'inception_3a_pool_bn',\n",
    "  'inception_3a_5x5_conv1', 'inception_3a_5x5_conv2', 'inception_3a_5x5_bn1', 'inception_3a_5x5_bn2',\n",
    "  'inception_3a_3x3_conv1', 'inception_3a_3x3_conv2', 'inception_3a_3x3_bn1', 'inception_3a_3x3_bn2',\n",
    "  'inception_3b_3x3_conv1', 'inception_3b_3x3_conv2', 'inception_3b_3x3_bn1', 'inception_3b_3x3_bn2',\n",
    "  'inception_3b_5x5_conv1', 'inception_3b_5x5_conv2', 'inception_3b_5x5_bn1', 'inception_3b_5x5_bn2',\n",
    "  'inception_3b_pool_conv', 'inception_3b_pool_bn',\n",
    "  'inception_3b_1x1_conv', 'inception_3b_1x1_bn',\n",
    "  'inception_3c_3x3_conv1', 'inception_3c_3x3_conv2', 'inception_3c_3x3_bn1', 'inception_3c_3x3_bn2',\n",
    "  'inception_3c_5x5_conv1', 'inception_3c_5x5_conv2', 'inception_3c_5x5_bn1', 'inception_3c_5x5_bn2',\n",
    "  'inception_4a_3x3_conv1', 'inception_4a_3x3_conv2', 'inception_4a_3x3_bn1', 'inception_4a_3x3_bn2',\n",
    "  'inception_4a_5x5_conv1', 'inception_4a_5x5_conv2', 'inception_4a_5x5_bn1', 'inception_4a_5x5_bn2',\n",
    "  'inception_4a_pool_conv', 'inception_4a_pool_bn',\n",
    "  'inception_4a_1x1_conv', 'inception_4a_1x1_bn',\n",
    "  'inception_4e_3x3_conv1', 'inception_4e_3x3_conv2', 'inception_4e_3x3_bn1', 'inception_4e_3x3_bn2',\n",
    "  'inception_4e_5x5_conv1', 'inception_4e_5x5_conv2', 'inception_4e_5x5_bn1', 'inception_4e_5x5_bn2',\n",
    "  'inception_5a_3x3_conv1', 'inception_5a_3x3_conv2', 'inception_5a_3x3_bn1', 'inception_5a_3x3_bn2',\n",
    "  'inception_5a_pool_conv', 'inception_5a_pool_bn',\n",
    "  'inception_5a_1x1_conv', 'inception_5a_1x1_bn',\n",
    "  'inception_5b_3x3_conv1', 'inception_5b_3x3_conv2', 'inception_5b_3x3_bn1', 'inception_5b_3x3_bn2',\n",
    "  'inception_5b_pool_conv', 'inception_5b_pool_bn',\n",
    "  'inception_5b_1x1_conv', 'inception_5b_1x1_bn',\n",
    "  'dense_layer'\n",
    "]\n",
    "\"\"\"Define the shape for every convolution layer weight (keep in mind that this shape is not the same with the result)\"\"\"\n",
    "conv_shape = {\n",
    "  'conv1': [64, 3, 7, 7],\n",
    "  'conv2': [64, 64, 1, 1],\n",
    "  'conv3': [192, 64, 3, 3],\n",
    "  'dense_layer': [128,736],\n",
    "  'inception_3a_1x1_conv': [64, 192, 1, 1],\n",
    "  'inception_3a_pool_conv': [32, 192, 1, 1],\n",
    "  'inception_3a_5x5_conv1': [16, 192, 1, 1],\n",
    "  'inception_3a_5x5_conv2': [32, 16, 5, 5],\n",
    "  'inception_3a_3x3_conv1': [96, 192, 1, 1],\n",
    "  'inception_3a_3x3_conv2': [128, 96, 3, 3],\n",
    "  'inception_3b_3x3_conv1': [96, 256, 1, 1],\n",
    "  'inception_3b_3x3_conv2': [128, 96, 3, 3],\n",
    "  'inception_3b_5x5_conv1': [32, 256, 1, 1],\n",
    "  'inception_3b_5x5_conv2': [64, 32, 5, 5],\n",
    "  'inception_3b_pool_conv': [64, 256, 1, 1],\n",
    "  'inception_3b_1x1_conv': [64, 256, 1, 1],\n",
    "  'inception_3c_3x3_conv1': [128, 320, 1, 1],\n",
    "  'inception_3c_3x3_conv2': [256, 128, 3, 3],\n",
    "  'inception_3c_5x5_conv1': [32, 320, 1, 1],\n",
    "  'inception_3c_5x5_conv2': [64, 32, 5, 5],\n",
    "  'inception_4a_3x3_conv1': [96, 640, 1, 1],\n",
    "  'inception_4a_3x3_conv2': [192, 96, 3, 3],\n",
    "  'inception_4a_5x5_conv1': [32, 640, 1, 1,],\n",
    "  'inception_4a_5x5_conv2': [64, 32, 5, 5],\n",
    "  'inception_4a_pool_conv': [128, 640, 1, 1],\n",
    "  'inception_4a_1x1_conv': [256, 640, 1, 1],\n",
    "  'inception_4e_3x3_conv1': [160, 640, 1, 1],\n",
    "  'inception_4e_3x3_conv2': [256, 160, 3, 3],\n",
    "  'inception_4e_5x5_conv1': [64, 640, 1, 1],\n",
    "  'inception_4e_5x5_conv2': [128, 64, 5, 5],\n",
    "  'inception_5a_3x3_conv1': [96, 1024, 1, 1],\n",
    "  'inception_5a_3x3_conv2': [384, 96, 3, 3],\n",
    "  'inception_5a_pool_conv': [96, 1024, 1, 1],\n",
    "  'inception_5a_1x1_conv': [256, 1024, 1, 1],\n",
    "  'inception_5b_3x3_conv1': [96, 736, 1, 1],\n",
    "  'inception_5b_3x3_conv2': [384, 96, 3, 3],\n",
    "  'inception_5b_pool_conv': [96, 736, 1, 1],\n",
    "  'inception_5b_1x1_conv': [256, 736, 1, 1],\n",
    "}\n",
    "def variable(value, dtype = _FLOATX, name = None):\n",
    "    \"\"\"Converting usual variable into a tensor variable and initialize it.\n",
    "    \n",
    "    Arguments:\n",
    "    value -- the value for the tensor variable\n",
    "    dtype -- the variable type for the tensor variable, by default as float32\n",
    "    name -- name of the variable if desired\n",
    "    \n",
    "    Return:\n",
    "    v -- the tensor variable after initialized\"\"\"\n",
    "    \n",
    "    #convert the variable to np.asarray first just in case if the input is a variable\n",
    "    v = tf.Variable(np.asarray(value, dtype = dtype), name=name)\n",
    "    _get_session().run(v.initializer)\n",
    "    return v\n",
    "\n",
    "def shape(x):\n",
    "    \"\"\"Return the shape of the tensor\n",
    "    \n",
    "    Arguments:\n",
    "    x -- a tensor\n",
    "    \n",
    "    Returns:\n",
    "    x.get_shape() -- the shape of the given tensor\"\"\"\n",
    "    return x.get_shape()\n",
    "\n",
    "def square(x):\n",
    "    \"\"\"Square every value inside the tensor\n",
    "    \n",
    "    Arguments:\n",
    "    x -- a tensor\n",
    "    \n",
    "    Returns:\n",
    "    tf.square(x) -- a tensor with the value equals value of x squared\"\"\"\n",
    "    return tf.square(x)\n",
    "\n",
    "def zeros(shape, dtype = _FLOATX, name = None):\n",
    "    \"\"\"Create a variable of zeros with a given shape, type and name\n",
    "    \n",
    "    Arguments:\n",
    "    shape -- the shape of the variable\n",
    "    dtype -- the type of the value inside the variable, float32 will be the default\n",
    "    name -- the name of the variable, by default it will be None\n",
    "    \n",
    "    Returns:\n",
    "    a tensorflow variable with value all zero\"\"\"\n",
    "    return variable(np.zeros(shape), dtype, name)\n",
    "\n",
    "#def concatenate(tensors, axis = -1):\n",
    "#    \"\"\"Concat the tensors on the specified axis\n",
    "    \n",
    "#    Arguments:\n",
    "#    tensors -- a list of tensors that are wanted to be concatenated together\n",
    "#    axis -- the axis where they will be concatenated\n",
    "    \n",
    "#    Returns:\n",
    "#    the concatenated tensors on the given axis\n",
    "#    example: tf.concat([a,b], axis =0) where a and b shapes are [6,7] will give the result of shape\n",
    "#    [12,7]. Note that concat is different than stack, where tf.stack([a,b], axis =0) will give the result\n",
    "#    [2,6,7].\"\"\"\n",
    "#    if axis < 0:\n",
    "#       axis = axis % len(tensors[0].get_shape()) #-1 % 4 = 3\n",
    "#    return tf.concat(axis = axis, values = tensors)\n",
    "\n",
    "def LRN2D(x):\n",
    "    \"\"\"Return local response normalization layer of tensorflow, with alpha = 1e-4 and beta=0.75\n",
    "    The alpha will be the weights of the added value (normalization constant), \n",
    "    and the beta will be the the one the power of the normalization (contrasting constant).\n",
    "    The depth radius of the local reponse normalization will follow the default, which uses 5 channel\n",
    "    depth as a default\n",
    "    \n",
    "    arguments:\n",
    "    x -- the tensor to be normalized\n",
    "    \n",
    "    returns:\n",
    "    the local response normalized tensor\"\"\"\n",
    "    \n",
    "    return tf.nn.local_response_normalization(x, alpha = 1e-4, beta = 0.75)\n",
    "\n",
    "def conv2d_bn(x, layer = '', conv1Channel = None, cv1Filter = (1,1), cv1Strides = (1,1),\n",
    "             conv2Channel = None, cv2Filter = (3,3), cv2Strides = (1,1), padding = None):\n",
    "    \"\"\"Implement a layer of combination between 2d convolutions, batch normalization and relu\n",
    "    The convolution implemented will use a 'valid convolution'\n",
    "    \n",
    "    \n",
    "    arguments:\n",
    "    x -- input tensor\n",
    "    layer -- string for the naming of the layer inside the conv2d and bn\n",
    "    conv1Channel -- int for the desired number of channel for the conv1 output\n",
    "    cv1Filter -- set with two int, indicating respectively the height and the width of the layer1\n",
    "                conv filter\n",
    "    cv1Strides -- set with two int, indicating respectively the height and the width of the layer1\n",
    "                conv strides\n",
    "    conv2Channel -- int for the desired number of channel for the conv2 output. Setting it to None\n",
    "                    will not execute the conv2Channel\n",
    "    cv1Filter -- set with two int, indicating respectively the height and the width of the layer2\n",
    "                conv filter\n",
    "    cv1Strides -- set with two int, indicating respectively the height and the width of the layer2\n",
    "                conv strides\n",
    "    padding -- the number of the padding that will bee used between the first convolution and the\n",
    "                second convolution. Input can be one of the following:\n",
    "                - (int,int) : the first int will be the height pad (respeectively), and the second int will be the width pad\n",
    "                                (respectively)\n",
    "                - (int) : respectively pad the height and the width with the given integerr\n",
    "                - ((int,int), (int, int)) : pad the first tuple will be the pad of the height, with the format of (top,bottom)\n",
    "                    the second tuple will be the pad for the width, with the format of (left, right)\n",
    "                - None: will not execute the ZeroPadding2d\n",
    "                \n",
    "                \n",
    "    Returns:\n",
    "    x = the tensor after implemented with the desired convolution(s) and padding\"\"\"\n",
    "    \n",
    "    if conv1Channel == None:\n",
    "        return x\n",
    "    \n",
    "    num = ''\n",
    "    if(conv2Channel != None):\n",
    "        num = 1\n",
    "    #implement the first convolution, where it usually is the pointwise convolution by default (to implement bottleneck layer)\n",
    "    x = Conv2D(conv1Channel, cv1Filter, strides = cv1Strides, data_format = \"channels_first\", name = layer+\"_conv\"+str(num))(x)\n",
    "    x = BatchNormalization(axis = 1, epsilon = 0.00001, name = layer+\"_bn\"+str(num))(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    \n",
    "    #if padding is none, directly return the result, this is done in case we need 1 convolution but we need to pad the result of the first conv\n",
    "    if padding == None:\n",
    "        return x\n",
    "    x = ZeroPadding2D(padding, data_format = \"channels_first\")(x)\n",
    "    \n",
    "    #check whether it needs only one convolution\n",
    "    if(conv2Channel == None):\n",
    "        return x\n",
    "    \n",
    "    #implement the second convolution\n",
    "    num = 2\n",
    "    x = Conv2D(conv2Channel, cv2Filter, strides = cv2Strides, data_format = \"channels_first\", name = layer+\"_conv\"+str(num))(x)\n",
    "    x = BatchNormalization(axis = 1, epsilon = 0.00001, name = layer+\"_bn\"+str(num))(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    return x\n",
    "\n",
    "def load_weights():\n",
    "    \"\"\"Through the weights that is available inside the 'weights' folder. Each filter will be availabe in csv format and is comma separated.\n",
    "    There are several kinds of layer type:\n",
    "    - conv: there are w and b weights \n",
    "    - bn: there are w b m v weights\n",
    "    - dense: there are w b weights\n",
    "    \n",
    "    Arguments:\n",
    "    none, directly accessing the WEIGHTS and conv_shape global variables\n",
    "    \n",
    "    Returns:\n",
    "    weights -- dictionary of weights obtained from the csv\"\"\"\n",
    "    \n",
    "    folder = \"./weights\" #this format means that it is a folder\n",
    "    weights_dict = {} #initialize blank dictionary\n",
    "    pathname = {}\n",
    "    \n",
    "    for i in os.listdir(folder):\n",
    "        #get the pathname without the .csv\n",
    "        pathname[i.replace(\".csv\", \"\")] = folder+\"/\"+i\n",
    "    \n",
    "    #np.genfromtxt converts contents of text file to our desired format \n",
    "    for i in WEIGHTS:\n",
    "        if(os.path.isdir(i)):\n",
    "            continue\n",
    "            \n",
    "        if \"conv\" in i:\n",
    "            weights = np.genfromtxt(pathname[i+\"_w\"], delimiter = \",\", dtype = None)\n",
    "            #reshape annd transpose to channel_first form\n",
    "            weights = np.reshape(weights, conv_shape[i])\n",
    "            weights = np.transpose(weights, (2,3,1,0))\n",
    "            \n",
    "            bias = np.genfromtxt(pathname[i+\"_b\"], delimiter = \",\", dtype = None)\n",
    "            weights_dict[i] = [weights,bias]\n",
    "        elif \"bn\" in i:\n",
    "            weights = np.genfromtxt(pathname[i+\"_w\"], delimiter = \",\", dtype = None)\n",
    "            bias = np.genfromtxt(pathname[i+\"_b\"], delimiter = \",\", dtype = None)\n",
    "            mean = np.genfromtxt(pathname[i+\"_m\"], delimiter = \",\", dtype = None)\n",
    "            variance = np.genfromtxt(pathname[i+\"_v\"], delimiter = \",\", dtype = None)\n",
    "            weights_dict[i] = [weights,bias,mean,variance]\n",
    "        elif \"dense\" in i:\n",
    "            weights = np.genfromtxt(folder+\"dense_w.csv\", delimiter = \",\", dtype = None)\n",
    "            #reshape annd transpose to channel_first form\n",
    "            weights = np.reshape(weights, conv_shape[i])\n",
    "            weights = np.transpose(weights, (1,0))\n",
    "            \n",
    "            bias = np.genfromtxt(folder+\"dense_b.csv\", delimiter = \",\", dtype = None)\n",
    "            weights_dict[i] = [weights,bias]\n",
    "    return weights_dict\n",
    "def setWeightsToModel(FRModel):\n",
    "    \"\"\"Load the weights from the .csv files in weight (which was exporteed from Openface torch model) and set it into the Keras Model\n",
    "    \n",
    "    Arguments:\n",
    "    FRModel -- The Keras FR Model that we want to set the weights\n",
    "    \n",
    "    Return:\n",
    "    None, since the FRModel is a Keras Model (which is a Python Class), any changes inside the function will affect externally\"\"\"\n",
    "    \n",
    "    weights_dict = load_weights()\n",
    "    \n",
    "    for layerName in WEIGHTS:\n",
    "        if FRModel.get_layer(layerName):\n",
    "            FRModel.get_layer(layerName).set_weights(weights_dict[layerName])\n",
    "        else:\n",
    "            print(\"Error!\")\n",
    "            \n",
    "def load_dataset():\n",
    "    \"\"\"Load the dataset from the train_happy.h5 and test_happy.h5\n",
    "    \n",
    "    arguments:\n",
    "    None, directly accessing the files in 'datasets' directory\n",
    "    \n",
    "    return:\n",
    "    train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes -- np array of respective categories\"\"\"\n",
    "    train_dataset = h5py.File('datasets/train_happy.h5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "\n",
    "    test_dataset = h5py.File('datasets/test_happy.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "    \n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n",
    "\n",
    "def img_to_encoding(image_path, model):\n",
    "    \"\"\"Encode each image in database beforehand to save computation time\n",
    "    \n",
    "    arguments:\n",
    "    image -- the path of the image file\n",
    "    model -- the built keras model for FR\n",
    "    \n",
    "    Returns:\n",
    "    embedding -- 128 encoded number nparray based on the encoding of the FRModel\"\"\"\n",
    "    \n",
    "    imgBGR = cv2.imread(image_path,1) #use cv2 so that the datatype is directly numpy array\n",
    "    img = imgBGR[...,::-1] #flip to rgb\n",
    "    img = np.transpose(img,(2,0,1)) #to convert to channel_first format\n",
    "    img = np.around(img/255.0, decimals = 12) #divide by 255 to normalize and round it to only 12 decimal numbers\n",
    "    \n",
    "    #increase the dimension so it becomes [1,img.shape] shape\n",
    "    predict = np.array([img])\n",
    "    embedding = model.predict(predict)\n",
    "    return embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inceptionBlock3a(X):\n",
    "    \"\"\"Implement the 3a inception block of OpenFace. Here are the numbers\n",
    "    of the channels per filter:\n",
    "    - 1x1 = 64\n",
    "    - 3x3(reduce) = 96\n",
    "    - 3x3 = 128\n",
    "    - 5x5(reduce) = 16\n",
    "    - 5x5 = 32\n",
    "    - pool = max, 32p\n",
    "    \n",
    "    Arguments:\n",
    "    X = Input tensors from the previous process. Channel_first\n",
    "    \n",
    "    Returns:\n",
    "    Concatenated inception tensors in the order of 3x3, 5x5, pool and 1x1\"\"\"\n",
    "    \n",
    "    #doing the 3x3 first\n",
    "    #the first step is to reduce the dimension for easier convolution\n",
    "    Conv3x3 = conv2d_bn(X, layer = \"inception_3a_3x3\", \n",
    "                        conv1Channel = 96, cv1Filter=(1,1), cv1Strides = (1,1),\n",
    "                       conv2Channel = 128, cv2Filter = (3,3), cv2Strides = (1,1), \n",
    "                        padding = (1,1))\n",
    "    \n",
    "    #doing the 5x5 convolution next\n",
    "    #the first step is to reduce the dimension for easier convolution\n",
    "    Conv5x5 = conv2d_bn(X, layer = \"inception_3a_5x5\", conv1Channel = 16, \n",
    "                        cv1Filter=(1,1), cv1Strides = (1,1),\n",
    "                       conv2Channel = 32, cv2Filter = (5,5), \n",
    "                        cv2Strides = (1,1), padding = (2,2))\n",
    "    \n",
    "    #beginning the pool part\n",
    "    Pool = MaxPooling2D((3,3), 2)(X)\n",
    "    Pool = conv2d_bn(Pool, layer = \"inception_3a_pool\", conv1Channel = 32, \n",
    "                     cv1Filter=(1,1), cv1Strides = (1,1), \n",
    "                     padding = ((3,4), (3,4)))\n",
    "    \n",
    "    #Beginning the 1x1 convolution part\n",
    "    Conv1x1 = conv2d_bn(X, layer = \"inception_3a_1x1\", \n",
    "                        conv1Channel = 64, cv1Filter=(1,1), \n",
    "                        cv1Strides = (1,1))\n",
    "    \n",
    "    #concat everything and return\n",
    "    return concatenate([Conv3x3, Conv5x5, Pool, Conv1x1], axis = 1)\n",
    "    \n",
    "def inceptionBlock3b(X):\n",
    "    \"\"\"Implement the 3b inception block of OpenFace. Here are the numbers\n",
    "    of the channels per filter:\n",
    "    - 1x1 = 64\n",
    "    - 3x3(reduce) = 96\n",
    "    - 3x3 = 128\n",
    "    - 5x5(reduce) = 32\n",
    "    - 5x5 = 64\n",
    "    - pool = l2(average pooling), 64p\n",
    "    \n",
    "    Arguments:\n",
    "    X = Input tensors from the previous process. Channel_first\n",
    "    \n",
    "    Returns:\n",
    "    Concatenated inception tensors in the order of 3x3, 5x5, pool and 1x1\"\"\"\n",
    "    \n",
    "    #doing the 3x3 version\n",
    "    Conv3x3 = conv2d_bn(X, layer = \"inception_3b_3x3\", conv1Channel = 96, \n",
    "                        cv1Filter=(1,1), cv1Strides = (1,1),\n",
    "                       conv2Channel = 128, cv2Filter = (3,3), \n",
    "                        cv2Strides = (1,1), padding = (1,1))\n",
    "    \n",
    "    #doing the 5x5 version\n",
    "    Conv5x5 = conv2d_bn(X, layer = \"inception_3b_5x5\", conv1Channel = 32, cv1Filter=(1,1), \n",
    "                        cv1Strides = (1,1), conv2Channel = 64, cv2Filter = (5,5), \n",
    "                        cv2Strides = (1,1), padding = (2,2))\n",
    "    \n",
    "    #doing the pool version\n",
    "    Pool = AveragePooling2D((3,3), strides = (3,3))(X)\n",
    "    Pool = conv2d_bn(Pool, layer = \"inception_3b_pool\", conv1Channel = 64, \n",
    "                     cv1Filter=(1,1), cv1Strides = (1,1), padding = (4,4))\n",
    "    \n",
    "    Conv1x1 = conv2d_bn(X, layer = \"inception_3b_1x1\", conv1Channel = 64, cv1Filter=(1,1), cv1Strides = (1,1))\n",
    "    \n",
    "    return concatenate([Conv3x3, Conv5x5, Pool, Conv1x1], axis = 1)\n",
    "\n",
    "\n",
    "def inceptionBlock3c(X):\n",
    "    \"\"\"Implement the 3c inception block of OpenFace. Here are the numbers\n",
    "    of the channels per filter:\n",
    "    - 3x3(reduce) = 128\n",
    "    - 3x3 = 256, strides 2\n",
    "    - 5x5(reduce) = 32\n",
    "    - 5x5 = 64, strides 2\n",
    "    - pool = m(max pooling), 3x3, 2\n",
    "    \n",
    "    Arguments:\n",
    "    X = Input tensors from the previous process. Channel_first\n",
    "    \n",
    "    Returns:\n",
    "    Concatenated inception tensors in the order of 3x3, 5x5, pool\"\"\"\n",
    "        \n",
    "    Conv3x3 = conv2d_bn(X, layer = \"inception_3c_3x3\", conv1Channel = 128, \n",
    "                        cv1Filter=(1,1), cv1Strides = (1,1),\n",
    "                       conv2Channel = 256, cv2Filter = (3,3), \n",
    "                        cv2Strides = (2,2), padding = (1,1))\n",
    "    Conv5x5 = conv2d_bn(X, layer = \"inception_3c_5x5\", conv1Channel = 32, \n",
    "                        cv1Filter=(1,1), cv1Strides = (1,1),\n",
    "                       conv2Channel = 64, cv2Filter = (5,5), \n",
    "                        cv2Strides = (2,2), padding = (2,2))\n",
    "    \n",
    "    #beginning the pooling part\n",
    "    Pool = MaxPooling2D((3,3), strides = (2,2))(X)\n",
    "    Pool = ZeroPadding2D(((0,1), (0,1)))(Pool)\n",
    "    return concatenate([Conv3x3, Conv5x5, Pool], axis = 1)\n",
    "\n",
    "def inceptionBlock4a(X):\n",
    "    \"\"\"Implement the 4a inception blcok of OpenFace. Here are the numbers\n",
    "    of the channels per filter:\n",
    "    - 1x1 = 256\n",
    "    - 3x3(reduce) = 96\n",
    "    - 3x3 = 192\n",
    "    - 5x5(reduce) = 32\n",
    "    - 5x5 = 64\n",
    "    - pool = l2(average pooling), 128p\n",
    "    \n",
    "    Arguments:\n",
    "    X = Input tensors from the previous process. Channel_first\n",
    "    \n",
    "    Returns:\n",
    "    Concatenated inception tensors in the order of 3x3, 5x5, pool,1x1\"\"\"\n",
    "    \n",
    "    Conv3x3 =  conv2d_bn(X, layer = \"inception_4a_3x3\", conv1Channel = 96, \n",
    "                         cv1Filter=(1,1), cv1Strides = (1,1), conv2Channel = 192, cv2Filter = (3,3), \n",
    "                         cv2Strides = (1,1), padding = (1,1))\n",
    "    Conv5x5 = conv2d_bn(X, layer = \"inception_4a_5x5\", conv1Channel = 32, \n",
    "                        cv1Filter=(1,1), cv1Strides = (1,1), conv2Channel = 64, cv2Filter = (5,5), \n",
    "                        cv2Strides = (1,1), padding = (2,2))\n",
    "    \n",
    "    #beginning the pooling part\n",
    "    Pool = AveragePooling2D((3,3), strides = (3,3))(X)\n",
    "    Pool = conv2d_bn(Pool, layer = \"inception_4a_pool\", conv1Channel = 128, \n",
    "                     cv1Filter=(1,1), cv1Strides = (1,1), padding = (2,2))\n",
    "    \n",
    "    Conv1x1 = conv2d_bn(X, layer = \"inception_4a_1x1\", conv1Channel = 256, cv1Filter=(1,1), cv1Strides = (1,1))\n",
    "    \n",
    "    return concatenate([Conv3x3, Conv5x5, Pool, Conv1x1], axis = 1)\n",
    "def inceptionBlock4e(X):\n",
    "    \"\"\"Implement the 4e inception blcok of OpenFace. Here are the numbers\n",
    "    of the channels per filter:\n",
    "    - 3x3(reduce) = 160\n",
    "    - 3x3 = 256, strides = 2\n",
    "    - 5x5(reduce) = 64\n",
    "    - 5x5 = 128, strides = 2\n",
    "    - pool = m(max pooling), 3x3,2\n",
    "    \n",
    "    Arguments:\n",
    "    X = Input tensors from the previous process. Channel_first\n",
    "    \n",
    "    Returns:\n",
    "    Concatenated inception tensors in the order of 3x3, 5x5, pool,1x1\"\"\"\n",
    "    \n",
    "    Conv3x3 =  conv2d_bn(X, layer = \"inception_4e_3x3\", conv1Channel = 160, \n",
    "                         cv1Filter=(1,1), cv1Strides = (1,1), conv2Channel = 256, \n",
    "                         cv2Filter = (3,3), cv2Strides = (2,2), padding = (1,1))\n",
    "    Conv5x5 = conv2d_bn(X, layer = \"inception_4e_5x5\", conv1Channel = 64, cv1Filter=(1,1), \n",
    "                        cv1Strides = (1,1), conv2Channel = 128, cv2Filter = (5,5), \n",
    "                        cv2Strides = (2,2), padding = (2,2))\n",
    "    \n",
    "    #beginning the pooling part\n",
    "    Pool = MaxPooling2D((3,3), strides = (2,2))(X)\n",
    "    Pool = ZeroPadding2D(((0,1), (0,1)))(Pool)\n",
    "    \n",
    "    return concatenate([Conv3x3, Conv5x5, Pool], axis = 1)\n",
    "def inceptionBlock5a(X):\n",
    "    \"\"\"Implement the 5a inception blcok of OpenFace. Here are the numbers\n",
    "    of the channels per filter:\n",
    "    - 1x1 = 256\n",
    "    - 3x3(reduce) = 96\n",
    "    - 3x3 = 384\n",
    "    - pool = l2(average pooling), 96p\n",
    "    \n",
    "    Arguments:\n",
    "    X = Input tensors from the previous process. Channel_first\n",
    "    \n",
    "    Returns:\n",
    "    Concatenated inception tensors in the order of 3x3, pool,1x1\"\"\"\n",
    "    \n",
    "    Conv3x3 =  conv2d_bn(X, layer = \"inception_5a_3x3\", conv1Channel = 96, \n",
    "                         cv1Filter=(1,1), cv1Strides = (1,1), conv2Channel = 384, \n",
    "                         cv2Filter = (3,3), cv2Strides = (1,1), padding = (1,1))\n",
    "    \n",
    "    #beginning the pooling part\n",
    "    Pool = AveragePooling2D((3,3), strides = (3,3))(X)\n",
    "    Pool = conv2d_bn(Pool, layer = \"inception_5a_Pool\", conv1Channel = 96, \n",
    "                     cv1Filter=(1,1), cv1Strides = (1,1), padding = (1,1))\n",
    "    \n",
    "    Conv1x1 = conv2d_bn(X, layer = \"inception_5a_1x1\", conv1Channel = 256, \n",
    "                        cv1Filter=(1,1), cv1Strides = (1,1))\n",
    "    \n",
    "    return concatenate([Conv3x3, Pool, Conv1x1], axis = 1)\n",
    "def inceptionBlock5b(X):\n",
    "    \"\"\"Implement the 5b inception blcok of OpenFace. Here are the numbers\n",
    "    of the channels per filter:\n",
    "    - 1x1 = 256\n",
    "    - 3x3(reduce) = 96\n",
    "    - 3x3 = 384\n",
    "    - pool = m(max pooling), 96p\n",
    "    \n",
    "    Arguments:\n",
    "    X = Input tensors from the previous process. Channel_first\n",
    "    \n",
    "    Returns:\n",
    "    Concatenated inception tensors in the order of 3x3, pool, 1x1\"\"\"\n",
    "    \n",
    "    Conv3x3 =  conv2d_bn(X, layer = \"inception_5b_3x3\", conv1Channel = 96, \n",
    "                         cv1Filter=(1,1), cv1Strides = (1,1), conv2Channel = 384, \n",
    "                         cv2Filter = (3,3), cv2Strides = (1,1), padding = (1,1))\n",
    "    \n",
    "    #beginning the pooling part\n",
    "    Pool = MaxPooling2D((3,3), strides = (2,2))(X)\n",
    "    Pool = conv2d_bn(Pool, layer = \"inception_5b_Pool\", conv1Channel = 96, \n",
    "                     cv1Filter=(1,1), cv1Strides = (1,1), padding = (1,1))\n",
    "    \n",
    "    Conv1x1 = conv2d_bn(X, layer = \"inception_5b_1x1\", conv1Channel = 256, \n",
    "                        cv1Filter=(1,1), cv1Strides = (1,1))\n",
    "    \n",
    "    return concatenate([Conv3x3, Pool, Conv1x1], axis = 1)\n",
    "def faceRecoModel(input_shape):\n",
    "    \"\"\"\n",
    "    Implemention of the Incepttion model used for FaceNet\n",
    "    \n",
    "    Arguments:\n",
    "    input_shape -- (channel,height,width)\n",
    "    \n",
    "    Returns:\n",
    "    FRModel -- FaceNet Keras implementation model\n",
    "    \n",
    "    since in the beginning we have set the data format to channel_first by default, we don't need to do anything\"\"\"\n",
    "    \n",
    "    X_input = Input(input_shape)\n",
    "    X = ZeroPadding2D((3,3))(X_input) #pad the beginning\n",
    "    \n",
    "    #begin the first block\n",
    "    X = Conv2D(64,(7,7), strides = (2,2), name = \"conv1\")(X)\n",
    "    X = BatchNormalization(axis = 1, name = \"bn1\")(X)\n",
    "    X = Activation(\"relu\")(X)\n",
    "    \n",
    "    #ZeroPadding + MaxPool\n",
    "    X = ZeroPadding2D((1,1))(X)\n",
    "    X = MaxPooling2D((3,3), strides = 2)(X)\n",
    "    \n",
    "    #begin the second Conv\n",
    "    X = Conv2D(64, (1,1), strides = (1,1), name = \"conv2\")(X)\n",
    "    X = BatchNormalization(axis = 1, epsilon = 0.00001, name = \"bn2\")(X)\n",
    "    X = Activation(\"relu\")(X)\n",
    "    \n",
    "    #ZeroPadding\n",
    "    X = ZeroPadding2D((1,1))(X)\n",
    "    \n",
    "    #begin the third conv\n",
    "    X = Conv2D(192, (3,3), name = \"conv3\")(X)\n",
    "    X = BatchNormalization(axis = 1, name = \"bn3\")(X)\n",
    "    X = Activation(\"relu\")(X)\n",
    "    \n",
    "    #ZeroPadding and Relu\n",
    "    X = ZeroPadding2D((1,1))(X)\n",
    "    X = MaxPooling2D((3,3), strides = 2)(X)\n",
    "    \n",
    "    #Beginning inception\n",
    "    X = inceptionBlock3a(X)\n",
    "    X = inceptionBlock3b(X)\n",
    "    X = inceptionBlock3c(X)\n",
    "    X = inceptionBlock4a(X)\n",
    "    X = inceptionBlock4e(X)\n",
    "    X = inceptionBlock5a(X)\n",
    "    X = inceptionBlock5b(X)\n",
    "    \n",
    "    \n",
    "    #beginning the final 3 layerrs\n",
    "    X = AveragePooling2D((3,3), strides = (1,1))(X)\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(128, name = \"dense_layer\")(X)\n",
    "    \n",
    "    # L2 normalization\n",
    "    X = Lambda(lambda  x: K.l2_normalize(x,axis=1))(X)\n",
    "    \n",
    "    model = Model(inputs = X_input, outputs = X)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Hp\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Hp\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4074: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "Total Params:  3743280\n"
     ]
    }
   ],
   "source": [
    "#The detail of the face recognition model will be implemented inside the inception_blocks_v2.py\n",
    "FRmodel = faceRecoModel(input_shape = (3,96,96)) #since now the order will be channel_first, the 3 channel will be placed on the beginning\n",
    "print(\"Total Params: \", FRmodel.count_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(y_true, y_pred, alpha = 0.2):\n",
    "    \"\"\"\n",
    "    Implementation of the triplet loss used to measure the squared l2-norm of anchor-positive and anchor-negative\n",
    "    \n",
    "    Arguments:\n",
    "    y_true -- True labels that is required when we define our own custom loss function (although it is left unused)\n",
    "    y_pred -- python list containing three objects:\n",
    "            anchor -- the encodings for the anchor images, of shape (None, 128)\n",
    "            positive -- the encodings for the positive images, of shape (None, 128)\n",
    "            negative -- the encodings for the negative images, of shape (None, 128)\n",
    "    alpha -- alpha to be inserted inside the triple loss function, set to 0.2 by default\n",
    "    \n",
    "    Note: the None right here symbolizes the amount of the data\n",
    "    \n",
    "    Returns:\n",
    "    loss -- real number, value of the loss\n",
    "    \"\"\"\n",
    "    \n",
    "    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]\n",
    "    \n",
    "    l2Pos = tf.reduce_sum(tf.square(anchor-positive),axis = -1)\n",
    "    \n",
    "    l2Neg = tf.reduce_sum(tf.square(anchor-negative), axis = -1)\n",
    "    \n",
    "    lossAmount  = tf.reduce_sum(tf.maximum(l2Pos-l2Neg+alpha, 0.0))\n",
    "    \n",
    "    return lossAmount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 528.1432\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as test:\n",
    "    tf.set_random_seed(1)\n",
    "    y_true = (None, None, None)\n",
    "    y_pred = (tf.random_normal([3, 128], mean=6, stddev=0.1, seed = 1),\n",
    "              tf.random_normal([3, 128], mean=1, stddev=1, seed = 1),\n",
    "              tf.random_normal([3, 128], mean=3, stddev=4, seed = 1))\n",
    "    loss = triplet_loss(y_true, y_pred)\n",
    "    \n",
    "    print(\"loss = \" + str(loss.eval()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-24b18e4d8646>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mFRmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtriplet_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msetWeightsToModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFRmodel\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#this function is already defined inside the frutils, basically it set the weights of the layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-140813da9cbe>\u001b[0m in \u001b[0;36msetWeightsToModel\u001b[1;34m(FRModel)\u001b[0m\n\u001b[0;32m    272\u001b[0m     None, since the FRModel is a Keras Model (which is a Python Class), any changes inside the function will affect externally\"\"\"\n\u001b[0;32m    273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 274\u001b[1;33m     \u001b[0mweights_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mlayerName\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mWEIGHTS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-140813da9cbe>\u001b[0m in \u001b[0;36mload_weights\u001b[1;34m()\u001b[0m\n\u001b[0;32m    244\u001b[0m             \u001b[1;31m#reshape annd transpose to channel_first form\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m             \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconv_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 246\u001b[1;33m             \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m             \u001b[0mbias\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpathname\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"_b\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\",\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "FRmodel.compile(optimizer = 'adam', loss = triplet_loss, metrics = ['accuracy'])\n",
    "setWeightsToModel(FRmodel) #this function is already defined inside the frutils, basically it set the weights of the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode the coding first beforehand\n",
    "database = {}\n",
    "database[\"danielle\"] = img_to_encoding(\"images/danielle.png\", FRmodel)\n",
    "database[\"younes\"] = img_to_encoding(\"images/younes.jpg\", FRmodel)\n",
    "database[\"tian\"] = img_to_encoding(\"images/tian.jpg\", FRmodel)\n",
    "database[\"andrew\"] = img_to_encoding(\"images/andrew.jpg\", FRmodel)\n",
    "database[\"kian\"] = img_to_encoding(\"images/kian.jpg\", FRmodel)\n",
    "database[\"dan\"] = img_to_encoding(\"images/dan.jpg\", FRmodel)\n",
    "database[\"sebastiano\"] = img_to_encoding(\"images/sebastiano.jpg\", FRmodel)\n",
    "database[\"bertrand\"] = img_to_encoding(\"images/bertrand.jpg\", FRmodel)\n",
    "database[\"kevin\"] = img_to_encoding(\"images/kevin.jpg\", FRmodel)\n",
    "database[\"felix\"] = img_to_encoding(\"images/felix.jpg\", FRmodel)\n",
    "database[\"benoit\"] = img_to_encoding(\"images/benoit.jpg\", FRmodel)\n",
    "database[\"arnaud\"] = img_to_encoding(\"images/arnaud.jpg\", FRmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make the verify function for face verification\n",
    "def verify(image_path, identity, database, model):\n",
    "    \"\"\"face verification function to make sure that the person inside the image matches the identity in the database. \n",
    "    \n",
    "     Arguments:\n",
    "    image_path -- path to an image\n",
    "    identity -- string, name of the person you'd like to verify the identity. Has to be an employee who works in the office.\n",
    "    database -- python dictionary mapping names of allowed people's names (strings) to their encodings (vectors).\n",
    "    model -- your Inception model instance in Keras\n",
    "    \n",
    "    Returns:\n",
    "    dist -- distance between the image_path and the image of \"identity\" in the database.\n",
    "    door_open -- True, if the door should open. False otherwise.\n",
    "    \"\"\"\n",
    "    \n",
    "    #encode the image first\n",
    "    toBeVerified = img_to_encoding(image_path, model)\n",
    "    \n",
    "    #count the l2 distance to check if it is lower than a threshold (0.7)\n",
    "    distance = np.linalg.norm(toBeVerified - database[identity])\n",
    "    \n",
    "    if(distance < 0.7):\n",
    "        print(f\"Hi {identity}, welcome in!\")\n",
    "        door_open = True\n",
    "    else:\n",
    "        print(f\"You're not {identity}, shoo!\")\n",
    "        door_open = False\n",
    "    \n",
    "    return distance, door_open\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify(\"images/camera_0.jpg\", \"younes\", database, FRmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image= Image.open(\"images/camera_2.jpg\")\n",
    "plt.imshow(image)\n",
    "verify(\"images/camera_2.jpg\", \"kian\", database, FRmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognition(images_path, database, model):\n",
    "    \"\"\"\n",
    "    Implements face recognition for the office by finding who is the person on the image_path image.\n",
    "    \n",
    "    Arguments:\n",
    "    image_path -- path to an image\n",
    "    database -- dictionary containing image encodings along with the name of the person on the image\n",
    "    model -- your Inception model instance in Keras\n",
    "    \n",
    "    Returns:\n",
    "    min_dist -- the minimum distance between image_path encoding and the encodings from the database\n",
    "    identity -- string, the name prediction for the person on image_path\n",
    "    \"\"\"\n",
    "    \n",
    "    encoded = img_to_encoding(images_path,model)\n",
    "    \n",
    "    min_dist = 100 #for now let's just set to 100\n",
    "    nameFinal = None\n",
    "    for name,face_encode in database.items():\n",
    "        distance = np.linalg.norm(encoded-face_encode)\n",
    "        if(distance < min_dist):\n",
    "            min_dist = distance\n",
    "            nameFinal = name\n",
    "            \n",
    "    if min_dist > 0.7:\n",
    "        print(\"Not recognized\")\n",
    "    else:\n",
    "        print(f\"Hi {nameFinal}!\")\n",
    "    return min_dist, nameFinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image= Image.open(\"images/camera_0.jpg\")\n",
    "plt.imshow(image)\n",
    "recognition(\"images/camera_0.jpg\", database, FRmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in os.listdir(\"./weights\"):\n",
    "    if(os.path.isfile(i)):\n",
    "        print(i)\n",
    "    else:\n",
    "        print(\"False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in os.listdir():\n",
    "    print(os.path.isdir(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dog():\n",
    "    def __init__(self, a):\n",
    "        self.leg = a\n",
    "def changingValue(a):\n",
    "    a.leg = 10\n",
    "def changingValue(a):\n",
    "    a.leg = 2\n",
    "a = Dog(1)\n",
    "changingValue(a)\n",
    "print(a.leg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
